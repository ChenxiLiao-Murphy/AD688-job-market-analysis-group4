[
  {
    "objectID": "ml_methods.html",
    "href": "ml_methods.html",
    "title": "ML Methods",
    "section": "",
    "text": "ML Methods\nThis page describes the machine learning methods applied to analyze the job market data.\nContent will be added here."
  },
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "This page provides an overview of the data cleaning and initial exploration process for the job market dataset.\nContent will be added here.\n\n\n\n\nCode\nimport pandas as pd\ndf1 = pd.read_csv(\"./data/lightcast_job_postings.csv\")\n\n\n\n\nCode\ndf1.head()\ndf1.info()\ndf1.describe()\n\n\n\n\n\n\n\nCode\ndf1.columns.tolist()\nprint(df1.columns.tolist())\n\n\n\n\n\n\n# Define columns that are irrelevant or redundant for our analysis\ncolumns_to_drop = [\n    # Tracking and metadata\n    \"ID\", \"LAST_UPDATED_DATE\", \"LAST_UPDATED_TIMESTAMP\", \"DUPLICATES\",\n    \"URL\", \"ACTIVE_URLS\", \"ACTIVE_SOURCES_INFO\", \"SOURCE_TYPES\", \"SOURCES\",\n\n    # Company raw info\n    \"COMPANY_RAW\", \"COMPANY_IS_STAFFING\",\n\n    # Raw or text-heavy fields\n    \"TITLE_RAW\", \"BODY\",\n\n    # Modeled / derived fields\n    \"MODELED_EXPIRED\", \"MODELED_DURATION\",\n\n    # Educational levels (redundant versions)\n    \"EDUCATION_LEVELS\", \"EDUCATION_LEVELS_NAME\",\n    \"MIN_EDULEVELS\", \"MIN_EDULEVELS_NAME\", \"MAX_EDULEVELS\",\n\n    # Redundant NAICS / SOC codes\n    \"NAICS_2022_2\", \"NAICS_2022_2_NAME\",\n    \"NAICS_2022_3\", \"NAICS_2022_3_NAME\",\n    \"SOC_2\", \"SOC_3\", \"SOC_5\"\n]\n\n# Drop columns, ignore if a column is missing\ndf1.drop(columns=columns_to_drop, inplace=True, errors=\"ignore\")\n\n# Display the first few rows to confirm\ndf1.head()\n\n\n\n\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nos.makedirs(\"figureswxw\", exist_ok=True)\n\n\n\n\nCode\nimport missingno as msno\nimport matplotlib.pyplot as plt\nmsno.heatmap(df1)\n\nplt.title(\"Missing Values Heatmap\")\nplt.tight_layout()\nplt.savefig(\"figureswxw/missing_values_heatmap.png\", dpi=300)\nplt.show()\n\n\n\n\n\n\n\n\n\nCode\n# Drop columns with &gt;50% missing values\ndf1.dropna(axis=1, thresh=len(df1) * 0.5, inplace=True)\n\n\nif \"SALARY\" in df1.columns:\n    df1[\"SALARY\"] = df1[\"SALARY\"].fillna(df1[\"SALARY\"].median())\n\n    df1[\"DURATION\"] = df1[\"DURATION\"].fillna(df1[\"DURATION\"].median())\n\ncategorical_columns = [\"REMOTE_TYPE_NAME\", \"COMPANY_NAME\", \"MAX_EDULEVELS_NAME\"]\n\nfor col in categorical_columns:\n    if col in df1.columns:\n        df1[col] = df1[col].fillna(\"Unknown\")\n\n\ndf1.info()\n\n\n\n\n\n\ndf1.drop_duplicates(subset=[\"TITLE_CLEAN\", \"COMPANY_NAME\", \"CITY_NAME\", \"POSTED\"], inplace=True)\n\ndf1[\"REMOTE_TYPE_NAME\"].value_counts(dropna=False)\ndf1[\"EMPLOYMENT_TYPE_NAME\"].value_counts(dropna=False)\n\n\n\nCode\n#improve\n\ndf1[\"EMPLOYMENT_TYPE_NAME\"] = df1[\"EMPLOYMENT_TYPE_NAME\"].replace({\n    \"Part-time (â‰¤ 32 hours)\": \"Part-time (≤ 32 hours)\",\n    \"Part-time / full-time\": \"Part-time / Full-time\"\n})\ndf1[\"EMPLOYMENT_TYPE_NAME\"] = df1[\"EMPLOYMENT_TYPE_NAME\"].fillna(\"Unknown\")\ndf1[\"EMPLOYMENT_TYPE_NAME\"].value_counts()"
  },
  {
    "objectID": "data_cleaning.html#lode-dataset",
    "href": "data_cleaning.html#lode-dataset",
    "title": "Data Cleaning",
    "section": "",
    "text": "/var/folders/4f/s815nnpx1_15rfdwxj6cwrtr0000gn/T/ipykernel_92605/3038307670.py:2: DtypeWarning: Columns (19,30) have mixed types. Specify dtype option on import or set low_memory=False.\n  df1 = pd.read_csv(\"./data/lightcast_job_postings.csv\")\n\n\n\ndf1.head()\ndf1.info()\ndf1.describe()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 72498 entries, 0 to 72497\nColumns: 131 entries, ID to NAICS_2022_6_NAME\ndtypes: float64(38), object(93)\nmemory usage: 72.5+ MB\n\n\n\n\n\n\n\n\n\nDUPLICATES\nDURATION\nMODELED_DURATION\nCOMPANY\nMIN_EDULEVELS\nMAX_EDULEVELS\nEMPLOYMENT_TYPE\nMIN_YEARS_EXPERIENCE\nMAX_YEARS_EXPERIENCE\nSALARY\n...\nLOT_OCCUPATION_GROUP\nLOT_V6_SPECIALIZED_OCCUPATION\nLOT_V6_OCCUPATION\nLOT_V6_OCCUPATION_GROUP\nLOT_V6_CAREER_AREA\nNAICS_2022_2\nNAICS_2022_3\nNAICS_2022_4\nNAICS_2022_5\nNAICS_2022_6\n\n\n\n\ncount\n72476.000000\n45182.000000\n53208.000000\n7.245400e+04\n72454.000000\n16315.000000\n72454.000000\n49352.000000\n8430.000000\n30808.000000\n...\n72454.000000\n7.245400e+04\n72454.000000\n72454.000000\n72454.000000\n72454.000000\n72454.000000\n72454.000000\n72454.000000\n72454.000000\n\n\nmean\n1.081627\n22.322695\n19.737615\n3.702704e+07\n31.482527\n2.833834\n1.058768\n5.486444\n3.773903\n117953.755031\n...\n2239.204475\n2.239318e+07\n223931.694096\n2239.204475\n22.281158\n58.352555\n587.864590\n5883.121995\n58834.317125\n588345.683937\n\n\nstd\n2.807512\n14.359085\n12.963769\n3.015089e+07\n44.747433\n0.584028\n0.286997\n3.322241\n2.576739\n45133.878359\n...\n285.424309\n2.854275e+06\n28542.747473\n285.424309\n2.854360\n18.626415\n186.259064\n1864.093904\n18642.971892\n186431.744508\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000e+00\n0.000000\n1.000000\n1.000000\n0.000000\n0.000000\n15860.000000\n...\n1111.000000\n1.111101e+07\n111110.000000\n1111.000000\n11.000000\n11.000000\n111.000000\n1111.000000\n11115.000000\n111150.000000\n\n\n25%\n0.000000\n11.000000\n10.000000\n6.505993e+06\n2.000000\n3.000000\n1.000000\n3.000000\n2.000000\n84928.500000\n...\n2310.000000\n2.310101e+07\n231010.000000\n2310.000000\n23.000000\n52.000000\n522.000000\n5223.000000\n52232.000000\n522320.000000\n\n\n50%\n0.000000\n18.000000\n16.000000\n3.761516e+07\n2.000000\n3.000000\n1.000000\n5.000000\n3.000000\n116300.000000\n...\n2311.000000\n2.311131e+07\n231113.000000\n2311.000000\n23.000000\n54.000000\n541.000000\n5415.000000\n54151.000000\n541519.000000\n\n\n75%\n1.000000\n32.000000\n28.000000\n4.330689e+07\n99.000000\n3.000000\n1.000000\n8.000000\n5.000000\n145600.000000\n...\n2311.000000\n2.311131e+07\n231113.000000\n2311.000000\n23.000000\n56.000000\n561.000000\n5614.000000\n56149.000000\n561499.000000\n\n\nmax\n100.000000\n59.000000\n59.000000\n1.082365e+08\n99.000000\n4.000000\n3.000000\n15.000000\n14.000000\n500000.000000\n...\n2712.000000\n2.712111e+07\n271211.000000\n2712.000000\n27.000000\n99.000000\n999.000000\n9999.000000\n99999.000000\n999999.000000\n\n\n\n\n8 rows × 38 columns"
  },
  {
    "objectID": "data_cleaning.html#check-columns-information",
    "href": "data_cleaning.html#check-columns-information",
    "title": "Data Cleaning",
    "section": "",
    "text": "Code\ndf1.columns.tolist()\nprint(df1.columns.tolist())"
  },
  {
    "objectID": "data_cleaning.html#dropping-unnecessary-columns",
    "href": "data_cleaning.html#dropping-unnecessary-columns",
    "title": "Data Cleaning",
    "section": "",
    "text": "# Define columns that are irrelevant or redundant for our analysis\ncolumns_to_drop = [\n    # Tracking and metadata\n    \"ID\", \"LAST_UPDATED_DATE\", \"LAST_UPDATED_TIMESTAMP\", \"DUPLICATES\",\n    \"URL\", \"ACTIVE_URLS\", \"ACTIVE_SOURCES_INFO\", \"SOURCE_TYPES\", \"SOURCES\",\n\n    # Company raw info\n    \"COMPANY_RAW\", \"COMPANY_IS_STAFFING\",\n\n    # Raw or text-heavy fields\n    \"TITLE_RAW\", \"BODY\",\n\n    # Modeled / derived fields\n    \"MODELED_EXPIRED\", \"MODELED_DURATION\",\n\n    # Educational levels (redundant versions)\n    \"EDUCATION_LEVELS\", \"EDUCATION_LEVELS_NAME\",\n    \"MIN_EDULEVELS\", \"MIN_EDULEVELS_NAME\", \"MAX_EDULEVELS\",\n\n    # Redundant NAICS / SOC codes\n    \"NAICS_2022_2\", \"NAICS_2022_2_NAME\",\n    \"NAICS_2022_3\", \"NAICS_2022_3_NAME\",\n    \"SOC_2\", \"SOC_3\", \"SOC_5\"\n]\n\n# Drop columns, ignore if a column is missing\ndf1.drop(columns=columns_to_drop, inplace=True, errors=\"ignore\")\n\n# Display the first few rows to confirm\ndf1.head()"
  },
  {
    "objectID": "data_cleaning.html#handling-missing-values",
    "href": "data_cleaning.html#handling-missing-values",
    "title": "Data Cleaning",
    "section": "",
    "text": "Code\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nos.makedirs(\"figureswxw\", exist_ok=True)\n\n\n\n\nCode\nimport missingno as msno\nimport matplotlib.pyplot as plt\nmsno.heatmap(df1)\n\nplt.title(\"Missing Values Heatmap\")\nplt.tight_layout()\nplt.savefig(\"figureswxw/missing_values_heatmap.png\", dpi=300)\nplt.show()\n\n\n\n\n\n\n\n\n\nCode\n# Drop columns with &gt;50% missing values\ndf1.dropna(axis=1, thresh=len(df1) * 0.5, inplace=True)\n\n\nif \"SALARY\" in df1.columns:\n    df1[\"SALARY\"] = df1[\"SALARY\"].fillna(df1[\"SALARY\"].median())\n\n    df1[\"DURATION\"] = df1[\"DURATION\"].fillna(df1[\"DURATION\"].median())\n\ncategorical_columns = [\"REMOTE_TYPE_NAME\", \"COMPANY_NAME\", \"MAX_EDULEVELS_NAME\"]\n\nfor col in categorical_columns:\n    if col in df1.columns:\n        df1[col] = df1[col].fillna(\"Unknown\")\n\n\ndf1.info()"
  },
  {
    "objectID": "data_cleaning.html#remove-duplicates",
    "href": "data_cleaning.html#remove-duplicates",
    "title": "Data Cleaning",
    "section": "",
    "text": "df1.drop_duplicates(subset=[\"TITLE_CLEAN\", \"COMPANY_NAME\", \"CITY_NAME\", \"POSTED\"], inplace=True)\n\ndf1[\"REMOTE_TYPE_NAME\"].value_counts(dropna=False)\ndf1[\"EMPLOYMENT_TYPE_NAME\"].value_counts(dropna=False)\n\n\n\nCode\n#improve\n\ndf1[\"EMPLOYMENT_TYPE_NAME\"] = df1[\"EMPLOYMENT_TYPE_NAME\"].replace({\n    \"Part-time (â‰¤ 32 hours)\": \"Part-time (≤ 32 hours)\",\n    \"Part-time / full-time\": \"Part-time / Full-time\"\n})\ndf1[\"EMPLOYMENT_TYPE_NAME\"] = df1[\"EMPLOYMENT_TYPE_NAME\"].fillna(\"Unknown\")\ndf1[\"EMPLOYMENT_TYPE_NAME\"].value_counts()"
  },
  {
    "objectID": "data_cleaning.html#remote-type-distribution",
    "href": "data_cleaning.html#remote-type-distribution",
    "title": "Data Cleaning",
    "section": "2.1 Remote Type distribution",
    "text": "2.1 Remote Type distribution\n\n\nCode\nremote_counts = df1[\"REMOTE_TYPE_NAME\"].value_counts()\n\nplt.figure(figsize=(10,6))\nsns.barplot(\n    x=remote_counts.index, \n    y=remote_counts.values, \n    palette=\"Set2\"\n)\nplt.title(\"Remote Type Distribution\")\nplt.ylabel(\"Number of Job Postings\")\nplt.xlabel(\"Remote Type\")\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.savefig(\"figureswxw/remote_type_distribution.png\", dpi=300)\nplt.show()\n\n\n\n\n\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf1[\"IS_AI\"] = df1[\"NAICS_2022_6_NAME\"].fillna(\"\").str.contains(\"AI|Artificial Intelligence\", case=False) | \\\n               df1[\"LOT_OCCUPATION\"].fillna(\"\").str.contains(\"AI|Artificial Intelligence\", case=False)\n\ndf1[\"IS_AI\"] = df1[\"IS_AI\"].map({True: \"AI\", False: \"Non-AI\"})"
  },
  {
    "objectID": "data_cleaning.html#top-10-states-by-job-postings",
    "href": "data_cleaning.html#top-10-states-by-job-postings",
    "title": "Data Cleaning",
    "section": "2.2 Top 10 states by job postings",
    "text": "2.2 Top 10 states by job postings\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf1 = pd.read_csv(\"data/lightcast_job_postings.csv\")\n\ncity_counts = df1[\"CITY_NAME\"].dropna().value_counts().head(10)\n\nplt.figure(figsize=(10,5))\nsns.barplot(x=city_counts.index, y=city_counts.values, palette=\"Accent\")\nplt.title(\"Top 10 Cities by Job Postings\")\nplt.ylabel(\"Number of Job Postings\")\nplt.xlabel(\"City\")\nplt.xticks(rotation=30)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.savefig(\"figureswxw/top_cities_job_postings.png\", dpi=300)\nplt.show()\n\n\n/var/folders/4f/s815nnpx1_15rfdwxj6cwrtr0000gn/T/ipykernel_8078/1316398421.py:6: DtypeWarning: Columns (19,30) have mixed types. Specify dtype option on import or set low_memory=False.\n  df1 = pd.read_csv(\"data/lightcast_job_postings.csv\")\n/var/folders/4f/s815nnpx1_15rfdwxj6cwrtr0000gn/T/ipykernel_8078/1316398421.py:11: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x=city_counts.index, y=city_counts.values, palette=\"Accent\")\n\n\n\n\n\nRemote type distribution"
  },
  {
    "objectID": "data_cleaning.html#top-10-cities-by-job-postings",
    "href": "data_cleaning.html#top-10-cities-by-job-postings",
    "title": "Data Cleaning",
    "section": "2.3 Top 10 cities by job postings",
    "text": "2.3 Top 10 cities by job postings\n\n\nCode\ncity_counts = df1[\"CITY_NAME\"].value_counts().head(10)\nplt.figure(figsize=(10,5))\nsns.barplot(x=city_counts.index, y=city_counts.values, palette=\"Accent\")\nplt.title(\"Top 10 Cities by Job Postings\")\nplt.ylabel(\"Number of Job Postings\")\nplt.xlabel(\"City\")\nplt.xticks(rotation=30)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.savefig(\"figureswxw/top_cities_job_postings.png\", dpi=300)\nplt.show()"
  },
  {
    "objectID": "data_cleaning.html#remote-type-by-top-5-states",
    "href": "data_cleaning.html#remote-type-by-top-5-states",
    "title": "Data Cleaning",
    "section": "2.4 Remote Type by Top 5 States",
    "text": "2.4 Remote Type by Top 5 States\n\n\nCode\ntop_states = df1[\"STATE_NAME\"].value_counts().head(5).index\ndf_top = df1[df1[\"STATE_NAME\"].isin(top_states)]\n\n\npivot = df_top.groupby([\"STATE_NAME\", \"REMOTE_TYPE_NAME\"]).size().unstack(fill_value=0)\n\nplt.figure(figsize=(10,6))\nsns.heatmap(pivot, annot=True, fmt=\"d\", cmap=\"Blues\")\nplt.title(\"Heatmap: Remote Type Count in Top 5 States\")\nplt.tight_layout()\nplt.savefig(\"figureswxw/heatmap_remote_type_top_states.png\", dpi=300)\nplt.show()"
  },
  {
    "objectID": "data_cleaning.html#industry-distribution-by-state",
    "href": "data_cleaning.html#industry-distribution-by-state",
    "title": "Data Cleaning",
    "section": "2.5 Industry Distribution by State",
    "text": "2.5 Industry Distribution by State\n\n\nCode\ntop_industries = df_top[\"NAICS_2022_6_NAME\"].value_counts().head(6).index\ndf_top[\"NAICS_GROUPED\"] = df_top[\"NAICS_2022_6_NAME\"].where(df_top[\"NAICS_2022_6_NAME\"].isin(top_industries), \"Other\")\n\n\npivot_industry = df_top.groupby([\"STATE_NAME\", \"NAICS_GROUPED\"]).size().unstack(fill_value=0)\n\npivot_industry.plot(kind=\"bar\", stacked=True, figsize=(12,8), colormap=\"Set1\")\nplt.title(\"Job Postings by Major Industry in Top 5 States\")\nplt.ylabel(\"Number of Job Postings\")\nplt.xlabel(\"State\")\nplt.xticks(rotation=0)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\nplt.legend(\n    title=\"NAICS_GROUPED\",\n    fontsize=10,\n    title_fontsize=10,\n    loc='upper center',\n    bbox_to_anchor=(0.5, -0.15),  \n    ncol=2,  \n    frameon=False  \n)\nplt.tight_layout()\nplt.savefig(\"figureswxw/industry_by_state_top_major.png\", dpi=300)\nplt.show()"
  },
  {
    "objectID": "data_cleaning.html#time-trend-of-remote-work-types",
    "href": "data_cleaning.html#time-trend-of-remote-work-types",
    "title": "Data Cleaning",
    "section": "2.4 Time Trend of Remote Work Types",
    "text": "2.4 Time Trend of Remote Work Types\n\n\nCode\nif \"POSTED\" in df1.columns:\n    df1[\"POSTED_DATE\"] = pd.to_datetime(df1[\"POSTED\"], errors='coerce')\n    df1 = df1.dropna(subset=[\"POSTED_DATE\"])\n    df1[\"POSTED_MONTH\"] = df1[\"POSTED_DATE\"].dt.to_period(\"M\")\n    \n    trend = df1.groupby([\"POSTED_MONTH\", \"REMOTE_TYPE_NAME\"]).size().unstack(fill_value=0)\n    \n    trend.plot(figsize=(14,7))\n    plt.title(\"Remote Work Trends Over Time\", fontsize=14)\n    plt.ylabel(\"Number of Job Postings\")\n    plt.xlabel(\"Month\")\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.savefig(\"figureswxw/remote_trend_over_time.png\", dpi=300)\n    plt.show()\nelse:\n    print(\"POSTED column not found in dataset.\")"
  },
  {
    "objectID": "data_cleaning.html#load-dataset",
    "href": "data_cleaning.html#load-dataset",
    "title": "Data Cleaning",
    "section": "",
    "text": "Code\nimport pandas as pd\ndf1 = pd.read_csv(\"./data/lightcast_job_postings.csv\")\n\n\n\n\nCode\ndf1.head()\ndf1.info()\ndf1.describe()"
  },
  {
    "objectID": "data_cleaning.html#top-10-states-ai-vs-non-ai-job-postings",
    "href": "data_cleaning.html#top-10-states-ai-vs-non-ai-job-postings",
    "title": "Data Cleaning",
    "section": "2.2 Top 10 states : AI vs Non-AI Job Postings",
    "text": "2.2 Top 10 states : AI vs Non-AI Job Postings\n\n\nCode\ntop_states = df1[\"STATE_NAME\"].value_counts().head(10).index\ndf_top_states = df1[df1[\"STATE_NAME\"].isin(top_states)]\n\npivot_states = df_top_states.groupby([\"STATE_NAME\", \"IS_AI\"]).size().unstack(fill_value=0)\n\npivot_states.plot(kind=\"bar\", stacked=True, figsize=(12,6), colormap=\"Set3\")\nplt.title(\"Top 10 States: AI vs Non-AI Job Postings\")\nplt.ylabel(\"Number of Job Postings\")\nplt.xlabel(\"State\")\nplt.xticks(rotation=30)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.savefig(\"figureswxw/top_states_ai_nonai.png\", dpi=300)\nplt.show()"
  },
  {
    "objectID": "data_cleaning.html#top-10-cities-ai-vs-non-ai-job-postings",
    "href": "data_cleaning.html#top-10-cities-ai-vs-non-ai-job-postings",
    "title": "Data Cleaning",
    "section": "2.3 Top 10 cities: AI vs Non-AI Job Postings",
    "text": "2.3 Top 10 cities: AI vs Non-AI Job Postings\n\n\nCode\ntop_cities = df1[\"CITY_NAME\"].value_counts().head(10).index\ndf_top_cities = df1[df1[\"CITY_NAME\"].isin(top_cities)]\n\npivot_cities = df_top_cities.groupby([\"CITY_NAME\", \"IS_AI\"]).size().unstack(fill_value=0)\n\npivot_cities.plot(kind=\"bar\", stacked=True, figsize=(12,6), colormap=\"Set1\")\nplt.title(\"Top 10 Cities: AI vs Non-AI Job Postings\")\nplt.ylabel(\"Number of Job Postings\")\nplt.xlabel(\"City\")\nplt.xticks(rotation=30)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.savefig(\"figureswxw/top_cities_ai_nonai.png\", dpi=300)\nplt.show()"
  },
  {
    "objectID": "data_cleaning.html#tech-hubs-vs-other-locations-hiring-trends",
    "href": "data_cleaning.html#tech-hubs-vs-other-locations-hiring-trends",
    "title": "Data Cleaning",
    "section": "2.5 Tech Hubs vs Other Locations Hiring Trends",
    "text": "2.5 Tech Hubs vs Other Locations Hiring Trends\n\n\nCode\ndf1[\"IS_HUB\"] = df1[\"CITY_NAME\"].apply(lambda x: \"Hub\" if x in [\"San Francisco\", \"Austin\", \"Boston\"] else \"Other\")\n\npivot_hub = df1.groupby([\"POSTED_MONTH\", \"IS_HUB\"]).size().unstack(fill_value=0)\n\npivot_hub.plot(figsize=(14,7))\nplt.title(\"Hiring Trends: Tech Hubs vs Other Locations\")\nplt.ylabel(\"Number of Job Postings\")\nplt.xlabel(\"Month\")\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.savefig(\"figureswxw/techhub_vs_other_trend.png\", dpi=300)\nplt.show()"
  },
  {
    "objectID": "data_cleaning.html#remote-job-trend-by-industry",
    "href": "data_cleaning.html#remote-job-trend-by-industry",
    "title": "Data Cleaning",
    "section": "2.6 Remote Job Trend by Industry",
    "text": "2.6 Remote Job Trend by Industry\n\n\nCode\ntop_industries = (\n    df1.groupby(\"NAICS_2022_6_NAME\").size()\n    .sort_values(ascending=False)\n    .head(10)\n    .index\n)\n\n\ndf_top_ind = df1[df1[\"NAICS_2022_6_NAME\"].isin(top_industries)]\n\n\ndf_top_ind[\"POSTED_DATE\"] = pd.to_datetime(df_top_ind[\"POSTED\"], errors='coerce')\ndf_top_ind = df_top_ind.dropna(subset=[\"POSTED_DATE\"])\ndf_top_ind[\"POSTED_MONTH\"] = df_top_ind[\"POSTED_DATE\"].dt.to_period(\"M\")\n\n\npivot = df_top_ind.groupby([\"POSTED_MONTH\", \"NAICS_2022_6_NAME\"]).size().unstack(fill_value=0)\n\npivot.plot(figsize=(14,7))\nplt.title(\"Remote Job Trends by Top 5 Industries Over Time\", fontsize=14)\nplt.ylabel(\"Number of Job Postings\")\nplt.xlabel(\"Month\")\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.legend(\n    title=\"NAICS_2022_6_NAME\",\n    loc='upper center',\n    bbox_to_anchor=(0.5, -0.15),  \n    ncol=2,                       \n    frameon=False\n)\nplt.tight_layout()\nplt.savefig(\"figureswxw/remote_trend_top5_industry.png\", dpi=300)\nplt.show()"
  },
  {
    "objectID": "data_cleaning.html#urbanrural-region-ai-vs-non-ai-job-postings",
    "href": "data_cleaning.html#urbanrural-region-ai-vs-non-ai-job-postings",
    "title": "Data Cleaning",
    "section": "2.7 Urban/Rural Region: AI vs Non-AI Job Postings",
    "text": "2.7 Urban/Rural Region: AI vs Non-AI Job Postings\n\n\nCode\nurban_cities = [\n    \"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"San Francisco\",\n    \"Austin\", \"Boston\", \"Dallas\", \"Seattle\", \"Washington\", \"Atlanta\"\n]\n\ndf1[\"CITY_NAME_CLEAN\"] = df1[\"CITY_NAME\"].str.split(\",\").str[0].str.strip().str.title()\n\ndf1[\"Urban_Rural\"] = df1[\"CITY_NAME_CLEAN\"].apply(\n    lambda x: \"Urban\" if x in urban_cities else \"Rural\"\n)\n\nprint(df1[\"Urban_Rural\"].value_counts())\n\n\n\n\nCode\nrequired_columns = [\"Urban_Rural\", \"IS_AI\"]\nmissing_columns = [col for col in required_columns if col not in df1.columns]\n\nif not missing_columns:\n\n    pivot_urban = df1.groupby([\"Urban_Rural\", \"IS_AI\"]).size().unstack(fill_value=0)\n\n    for region in pivot_urban.index:\n        data = pivot_urban.loc[region]\n        plt.figure(figsize=(6,6))\n        plt.pie(\n            data,\n            labels=data.index,\n            autopct='%1.1f%%',\n            startangle=90,\n            colors=[\"#ff9999\",\"#66b3ff\"],\n            wedgeprops={'edgecolor': 'black'}\n        )\n        plt.title(f\"{region} - AI vs Non-AI Job Postings\")\n        plt.tight_layout()\n        fname = f\"figureswxw/{region.lower()}_ai_nonai_pie.png\"\n        plt.savefig(fname, dpi=300)\n        plt.show()\n        print(f\"Chart saved as {fname}\")\nelse:\n    print(f\"The following required columns are missing: {missing_columns}. Please check your dataset.\")"
  },
  {
    "objectID": "data_cleaning_eda.html",
    "href": "data_cleaning_eda.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "This page provides an overview of the data cleaning and initial exploration process for the job market dataset.\nContent will be added here.\n\n\n\n\nCode\nimport pandas as pd\ndf1 = pd.read_csv(\"./data/lightcast_job_postings.csv\")\n\n\n\n\nCode\ndf1.head()\ndf1.info()\ndf1.describe()\n\n\n\n\n\n\n\nCode\ndf1.columns.tolist()\nprint(df1.columns.tolist())\n\n\n\n\n\n\n# Define columns that are irrelevant or redundant for our analysis\ncolumns_to_drop = [\n    # Tracking and metadata\n    \"ID\", \"LAST_UPDATED_DATE\", \"LAST_UPDATED_TIMESTAMP\", \"DUPLICATES\",\n    \"URL\", \"ACTIVE_URLS\", \"ACTIVE_SOURCES_INFO\", \"SOURCE_TYPES\", \"SOURCES\",\n\n    # Company raw info\n    \"COMPANY_RAW\", \"COMPANY_IS_STAFFING\",\n\n    # Raw or text-heavy fields\n    \"TITLE_RAW\", \"BODY\",\n\n    # Modeled / derived fields\n    \"MODELED_EXPIRED\", \"MODELED_DURATION\",\n\n    # Educational levels (redundant versions)\n    \"EDUCATION_LEVELS\", \"EDUCATION_LEVELS_NAME\",\n    \"MIN_EDULEVELS\", \"MIN_EDULEVELS_NAME\", \"MAX_EDULEVELS\",\n\n    # Redundant NAICS / SOC codes\n    \"NAICS_2022_2\", \"NAICS_2022_2_NAME\",\n    \"NAICS_2022_3\", \"NAICS_2022_3_NAME\",\n    \"SOC_2\", \"SOC_3\", \"SOC_5\"\n]\n\n# Drop columns, ignore if a column is missing\ndf1.drop(columns=columns_to_drop, inplace=True, errors=\"ignore\")\n\n# Display the first few rows to confirm\ndf1.head()\n\n\n\n\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nos.makedirs(\"figureswxw\", exist_ok=True)\n\n\n\n\nCode\nimport missingno as msno\nimport matplotlib.pyplot as plt\nmsno.heatmap(df1)\n\nplt.title(\"Missing Values Heatmap\")\nplt.tight_layout()\nplt.savefig(\"figureswxw/missing_values_heatmap.png\", dpi=300)\nplt.show()\n\n\n\n\n\n\n\n\n\nCode\n# Drop columns with &gt;50% missing values\ndf1.dropna(axis=1, thresh=len(df1) * 0.5, inplace=True)\n\n\nif \"SALARY\" in df1.columns:\n    df1[\"SALARY\"] = df1[\"SALARY\"].fillna(df1[\"SALARY\"].median())\n\n    df1[\"DURATION\"] = df1[\"DURATION\"].fillna(df1[\"DURATION\"].median())\n\ncategorical_columns = [\"REMOTE_TYPE_NAME\", \"COMPANY_NAME\", \"MAX_EDULEVELS_NAME\"]\n\nfor col in categorical_columns:\n    if col in df1.columns:\n        df1[col] = df1[col].fillna(\"Unknown\")\n\n\ndf1.info()\n\n\n\n\n\n\ndf1.drop_duplicates(subset=[\"TITLE_CLEAN\", \"COMPANY_NAME\", \"CITY_NAME\", \"POSTED\"], inplace=True)\n\ndf1[\"REMOTE_TYPE_NAME\"].value_counts(dropna=False)\ndf1[\"EMPLOYMENT_TYPE_NAME\"].value_counts(dropna=False)\n\n\n\nCode\n#improve\n\ndf1[\"EMPLOYMENT_TYPE_NAME\"] = df1[\"EMPLOYMENT_TYPE_NAME\"].replace({\n    \"Part-time (â‰¤ 32 hours)\": \"Part-time (≤ 32 hours)\",\n    \"Part-time / full-time\": \"Part-time / Full-time\"\n})\ndf1[\"EMPLOYMENT_TYPE_NAME\"] = df1[\"EMPLOYMENT_TYPE_NAME\"].fillna(\"Unknown\")\ndf1[\"EMPLOYMENT_TYPE_NAME\"].value_counts()"
  },
  {
    "objectID": "data_cleaning_eda.html#load-dataset",
    "href": "data_cleaning_eda.html#load-dataset",
    "title": "Data Cleaning",
    "section": "",
    "text": "Code\nimport pandas as pd\ndf1 = pd.read_csv(\"./data/lightcast_job_postings.csv\")\n\n\n\n\nCode\ndf1.head()\ndf1.info()\ndf1.describe()"
  },
  {
    "objectID": "data_cleaning_eda.html#check-columns-information",
    "href": "data_cleaning_eda.html#check-columns-information",
    "title": "Data Cleaning",
    "section": "",
    "text": "Code\ndf1.columns.tolist()\nprint(df1.columns.tolist())"
  },
  {
    "objectID": "data_cleaning_eda.html#dropping-unnecessary-columns",
    "href": "data_cleaning_eda.html#dropping-unnecessary-columns",
    "title": "Data Cleaning",
    "section": "",
    "text": "# Define columns that are irrelevant or redundant for our analysis\ncolumns_to_drop = [\n    # Tracking and metadata\n    \"ID\", \"LAST_UPDATED_DATE\", \"LAST_UPDATED_TIMESTAMP\", \"DUPLICATES\",\n    \"URL\", \"ACTIVE_URLS\", \"ACTIVE_SOURCES_INFO\", \"SOURCE_TYPES\", \"SOURCES\",\n\n    # Company raw info\n    \"COMPANY_RAW\", \"COMPANY_IS_STAFFING\",\n\n    # Raw or text-heavy fields\n    \"TITLE_RAW\", \"BODY\",\n\n    # Modeled / derived fields\n    \"MODELED_EXPIRED\", \"MODELED_DURATION\",\n\n    # Educational levels (redundant versions)\n    \"EDUCATION_LEVELS\", \"EDUCATION_LEVELS_NAME\",\n    \"MIN_EDULEVELS\", \"MIN_EDULEVELS_NAME\", \"MAX_EDULEVELS\",\n\n    # Redundant NAICS / SOC codes\n    \"NAICS_2022_2\", \"NAICS_2022_2_NAME\",\n    \"NAICS_2022_3\", \"NAICS_2022_3_NAME\",\n    \"SOC_2\", \"SOC_3\", \"SOC_5\"\n]\n\n# Drop columns, ignore if a column is missing\ndf1.drop(columns=columns_to_drop, inplace=True, errors=\"ignore\")\n\n# Display the first few rows to confirm\ndf1.head()"
  },
  {
    "objectID": "data_cleaning_eda.html#handling-missing-values",
    "href": "data_cleaning_eda.html#handling-missing-values",
    "title": "Data Cleaning",
    "section": "",
    "text": "Code\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nos.makedirs(\"figureswxw\", exist_ok=True)\n\n\n\n\nCode\nimport missingno as msno\nimport matplotlib.pyplot as plt\nmsno.heatmap(df1)\n\nplt.title(\"Missing Values Heatmap\")\nplt.tight_layout()\nplt.savefig(\"figureswxw/missing_values_heatmap.png\", dpi=300)\nplt.show()\n\n\n\n\n\n\n\n\n\nCode\n# Drop columns with &gt;50% missing values\ndf1.dropna(axis=1, thresh=len(df1) * 0.5, inplace=True)\n\n\nif \"SALARY\" in df1.columns:\n    df1[\"SALARY\"] = df1[\"SALARY\"].fillna(df1[\"SALARY\"].median())\n\n    df1[\"DURATION\"] = df1[\"DURATION\"].fillna(df1[\"DURATION\"].median())\n\ncategorical_columns = [\"REMOTE_TYPE_NAME\", \"COMPANY_NAME\", \"MAX_EDULEVELS_NAME\"]\n\nfor col in categorical_columns:\n    if col in df1.columns:\n        df1[col] = df1[col].fillna(\"Unknown\")\n\n\ndf1.info()"
  },
  {
    "objectID": "data_cleaning_eda.html#remove-duplicates",
    "href": "data_cleaning_eda.html#remove-duplicates",
    "title": "Data Cleaning",
    "section": "",
    "text": "df1.drop_duplicates(subset=[\"TITLE_CLEAN\", \"COMPANY_NAME\", \"CITY_NAME\", \"POSTED\"], inplace=True)\n\ndf1[\"REMOTE_TYPE_NAME\"].value_counts(dropna=False)\ndf1[\"EMPLOYMENT_TYPE_NAME\"].value_counts(dropna=False)\n\n\n\nCode\n#improve\n\ndf1[\"EMPLOYMENT_TYPE_NAME\"] = df1[\"EMPLOYMENT_TYPE_NAME\"].replace({\n    \"Part-time (â‰¤ 32 hours)\": \"Part-time (≤ 32 hours)\",\n    \"Part-time / full-time\": \"Part-time / Full-time\"\n})\ndf1[\"EMPLOYMENT_TYPE_NAME\"] = df1[\"EMPLOYMENT_TYPE_NAME\"].fillna(\"Unknown\")\ndf1[\"EMPLOYMENT_TYPE_NAME\"].value_counts()"
  },
  {
    "objectID": "data_cleaning_eda.html#remote-type-distribution",
    "href": "data_cleaning_eda.html#remote-type-distribution",
    "title": "Data Cleaning",
    "section": "2.1 Remote Type distribution",
    "text": "2.1 Remote Type distribution\n\n\nCode\nremote_counts = df1[\"REMOTE_TYPE_NAME\"].value_counts()\n\nplt.figure(figsize=(10,6))\nsns.barplot(\n    x=remote_counts.index, \n    y=remote_counts.values, \n    palette=\"Set2\"\n)\nplt.title(\"Remote Type Distribution\")\nplt.ylabel(\"Number of Job Postings\")\nplt.xlabel(\"Remote Type\")\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.savefig(\"figureswxw/remote_type_distribution.png\", dpi=300)\nplt.show()\n\n\n\n\n\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf1[\"IS_AI\"] = df1[\"NAICS_2022_6_NAME\"].fillna(\"\").str.contains(\"AI|Artificial Intelligence\", case=False) | \\\n               df1[\"LOT_OCCUPATION\"].fillna(\"\").str.contains(\"AI|Artificial Intelligence\", case=False)\n\ndf1[\"IS_AI\"] = df1[\"IS_AI\"].map({True: \"AI\", False: \"Non-AI\"})"
  },
  {
    "objectID": "data_cleaning_eda.html#top-10-states-ai-vs-non-ai-job-postings",
    "href": "data_cleaning_eda.html#top-10-states-ai-vs-non-ai-job-postings",
    "title": "Data Cleaning",
    "section": "2.2 Top 10 states : AI vs Non-AI Job Postings",
    "text": "2.2 Top 10 states : AI vs Non-AI Job Postings\n\n\nCode\ntop_states = df1[\"STATE_NAME\"].value_counts().head(10).index\ndf_top_states = df1[df1[\"STATE_NAME\"].isin(top_states)]\n\npivot_states = df_top_states.groupby([\"STATE_NAME\", \"IS_AI\"]).size().unstack(fill_value=0)\n\npivot_states.plot(kind=\"bar\", stacked=True, figsize=(12,6), colormap=\"Set3\")\nplt.title(\"Top 10 States: AI vs Non-AI Job Postings\")\nplt.ylabel(\"Number of Job Postings\")\nplt.xlabel(\"State\")\nplt.xticks(rotation=30)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.savefig(\"figureswxw/top_states_ai_nonai.png\", dpi=300)\nplt.show()"
  },
  {
    "objectID": "data_cleaning_eda.html#top-10-cities-ai-vs-non-ai-job-postings",
    "href": "data_cleaning_eda.html#top-10-cities-ai-vs-non-ai-job-postings",
    "title": "Data Cleaning",
    "section": "2.3 Top 10 cities: AI vs Non-AI Job Postings",
    "text": "2.3 Top 10 cities: AI vs Non-AI Job Postings\n\n\nCode\ntop_cities = df1[\"CITY_NAME\"].value_counts().head(10).index\ndf_top_cities = df1[df1[\"CITY_NAME\"].isin(top_cities)]\n\npivot_cities = df_top_cities.groupby([\"CITY_NAME\", \"IS_AI\"]).size().unstack(fill_value=0)\n\npivot_cities.plot(kind=\"bar\", stacked=True, figsize=(12,6), colormap=\"Set1\")\nplt.title(\"Top 10 Cities: AI vs Non-AI Job Postings\")\nplt.ylabel(\"Number of Job Postings\")\nplt.xlabel(\"City\")\nplt.xticks(rotation=30)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.savefig(\"figureswxw/top_cities_ai_nonai.png\", dpi=300)\nplt.show()"
  },
  {
    "objectID": "data_cleaning_eda.html#time-trend-of-remote-work-types",
    "href": "data_cleaning_eda.html#time-trend-of-remote-work-types",
    "title": "Data Cleaning",
    "section": "2.4 Time Trend of Remote Work Types",
    "text": "2.4 Time Trend of Remote Work Types\n\n\nCode\nif \"POSTED\" in df1.columns:\n    df1[\"POSTED_DATE\"] = pd.to_datetime(df1[\"POSTED\"], errors='coerce')\n    df1 = df1.dropna(subset=[\"POSTED_DATE\"])\n    df1[\"POSTED_MONTH\"] = df1[\"POSTED_DATE\"].dt.to_period(\"M\")\n    \n    trend = df1.groupby([\"POSTED_MONTH\", \"REMOTE_TYPE_NAME\"]).size().unstack(fill_value=0)\n    \n    trend.plot(figsize=(14,7))\n    plt.title(\"Remote Work Trends Over Time\", fontsize=14)\n    plt.ylabel(\"Number of Job Postings\")\n    plt.xlabel(\"Month\")\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.savefig(\"figureswxw/remote_trend_over_time.png\", dpi=300)\n    plt.show()\nelse:\n    print(\"POSTED column not found in dataset.\")"
  },
  {
    "objectID": "data_cleaning_eda.html#tech-hubs-vs-other-locations-hiring-trends",
    "href": "data_cleaning_eda.html#tech-hubs-vs-other-locations-hiring-trends",
    "title": "Data Cleaning",
    "section": "2.5 Tech Hubs vs Other Locations Hiring Trends",
    "text": "2.5 Tech Hubs vs Other Locations Hiring Trends\n\n\nCode\ndf1[\"IS_HUB\"] = df1[\"CITY_NAME\"].apply(lambda x: \"Hub\" if x in [\"San Francisco\", \"Austin\", \"Boston\"] else \"Other\")\n\npivot_hub = df1.groupby([\"POSTED_MONTH\", \"IS_HUB\"]).size().unstack(fill_value=0)\n\npivot_hub.plot(figsize=(14,7))\nplt.title(\"Hiring Trends: Tech Hubs vs Other Locations\")\nplt.ylabel(\"Number of Job Postings\")\nplt.xlabel(\"Month\")\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.savefig(\"figureswxw/techhub_vs_other_trend.png\", dpi=300)\nplt.show()"
  },
  {
    "objectID": "data_cleaning_eda.html#remote-job-trend-by-industry",
    "href": "data_cleaning_eda.html#remote-job-trend-by-industry",
    "title": "Data Cleaning",
    "section": "2.6 Remote Job Trend by Industry",
    "text": "2.6 Remote Job Trend by Industry\n\n\nCode\ntop_industries = (\n    df1.groupby(\"NAICS_2022_6_NAME\").size()\n    .sort_values(ascending=False)\n    .head(10)\n    .index\n)\n\n\ndf_top_ind = df1[df1[\"NAICS_2022_6_NAME\"].isin(top_industries)]\n\n\ndf_top_ind[\"POSTED_DATE\"] = pd.to_datetime(df_top_ind[\"POSTED\"], errors='coerce')\ndf_top_ind = df_top_ind.dropna(subset=[\"POSTED_DATE\"])\ndf_top_ind[\"POSTED_MONTH\"] = df_top_ind[\"POSTED_DATE\"].dt.to_period(\"M\")\n\n\npivot = df_top_ind.groupby([\"POSTED_MONTH\", \"NAICS_2022_6_NAME\"]).size().unstack(fill_value=0)\n\npivot.plot(figsize=(14,7))\nplt.title(\"Remote Job Trends by Top 5 Industries Over Time\", fontsize=14)\nplt.ylabel(\"Number of Job Postings\")\nplt.xlabel(\"Month\")\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.legend(\n    title=\"NAICS_2022_6_NAME\",\n    loc='upper center',\n    bbox_to_anchor=(0.5, -0.15),  \n    ncol=2,                       \n    frameon=False\n)\nplt.tight_layout()\nplt.savefig(\"figureswxw/remote_trend_top5_industry.png\", dpi=300)\nplt.show()"
  },
  {
    "objectID": "data_cleaning_eda.html#urbanrural-region-ai-vs-non-ai-job-postings",
    "href": "data_cleaning_eda.html#urbanrural-region-ai-vs-non-ai-job-postings",
    "title": "Data Cleaning",
    "section": "2.7 Urban/Rural Region: AI vs Non-AI Job Postings",
    "text": "2.7 Urban/Rural Region: AI vs Non-AI Job Postings\n\n\nCode\nurban_cities = [\n    \"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"San Francisco\",\n    \"Austin\", \"Boston\", \"Dallas\", \"Seattle\", \"Washington\", \"Atlanta\"\n]\n\ndf1[\"CITY_NAME_CLEAN\"] = df1[\"CITY_NAME\"].str.split(\",\").str[0].str.strip().str.title()\n\ndf1[\"Urban_Rural\"] = df1[\"CITY_NAME_CLEAN\"].apply(\n    lambda x: \"Urban\" if x in urban_cities else \"Rural\"\n)\n\nprint(df1[\"Urban_Rural\"].value_counts())\n\n\n\n2.7.1 Stacked Bar Chart\n\n\nCode\nif {\"Urban_Rural\", \"IS_AI\"}.issubset(df1.columns):\n    pivot_urban = df1.groupby([\"Urban_Rural\", \"IS_AI\"]).size().unstack(fill_value=0)\n\n    ax = pivot_urban.plot(\n        kind=\"bar\",\n        stacked=True,\n        figsize=(8, 5),\n        color=[\"#ff9999\", \"#66b3ff\"],\n        edgecolor=\"black\"\n    )\n    ax.set_title(\"Urban and Rural Regions: AI vs Non-AI Job Postings\", fontsize=14)\n    ax.set_ylabel(\"Number of Job Postings\")\n    ax.set_xlabel(\"Region Type\")\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n    ax.grid(axis='y', linestyle='--', alpha=0.7)\n\n    plt.tight_layout()\n    plt.savefig(\"figureswxw/urban_rural_ai_nonai_bar.png\", dpi=300)\n    plt.show()\n\nelse:\n    print(\"Required columns are missing. Please check your dataset.\")\n\n\n\n\n\n\n\n\n\n2.7.2 Pie Chart\n\n\nCode\nimport matplotlib.pyplot as plt\n\nif {\"Urban_Rural\", \"IS_AI\"}.issubset(df1.columns):\n    pivot_urban = df1.groupby([\"Urban_Rural\", \"IS_AI\"]).size().unstack(fill_value=0)\n\n    fig, axes = plt.subplots(1, 2, figsize=(8, 4))  \n\n    for ax, region in zip(axes, [\"Urban\", \"Rural\"]):\n        data = pivot_urban.loc[region]\n        ax.pie(\n            data,\n            labels=data.index,\n            autopct='%1.1f%%',\n            startangle=90,\n            colors=[\"#ff9999\", \"#66b3ff\"],\n            wedgeprops={'edgecolor': 'black'}\n        )\n        ax.set_title(f\"{region} - AI vs Non-AI\")\n\n    plt.tight_layout()\n    plt.savefig(\"figureswxw/urban_rural_ai_nonai_pie_combined.png\", dpi=300)\n    plt.show()\n\nelse:\n    print(\"Required columns are missing. Please check your dataset.\")"
  },
  {
    "objectID": "eda_enhance.html",
    "href": "eda_enhance.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Exploratory Data Analysis\nThis page summarizes the exploratory data analysis (EDA) of the dataset, including key trends and patterns.\nContent will be added here.\nimport seaborn as sns import matplotlib.pyplot as plt\nplt.figure(figsize=(10,6)) sns.barplot(data=df, x=“STATE_NAME”, y=“Average_Salary”) plt.title(“Average Salary by State”) plt.xticks(rotation=45) plt.tight_layout() plt.savefig(“figures/salary_by_state.png”) plt.show()\n\nSalary by State"
  }
]