[
  {
    "objectID": "ml_methods.html",
    "href": "ml_methods.html",
    "title": "ML Methods",
    "section": "",
    "text": "ML Methods\nThis page describes the machine learning methods applied to analyze the job market data.\nContent will be added here."
  },
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "This page provides an overview of the data cleaning and initial exploration process for the job market dataset.\nContent will be added here.\n\n\n\n\nCode\nimport pandas as pd\ndf1 = pd.read_csv(\"./data/lightcast_job_postings.csv\")\n\n\n\n\nCode\ndf1.head()\ndf1.info()\ndf1.describe()\n\n\n\n\n\n\n\nCode\ndf1.columns.tolist()\nprint(df1.columns.tolist())\n\n\n\n\n\n\n# Define columns that are irrelevant or redundant for our analysis\ncolumns_to_drop = [\n    # Tracking and metadata\n    \"ID\", \"LAST_UPDATED_DATE\", \"LAST_UPDATED_TIMESTAMP\", \"DUPLICATES\",\n    \"URL\", \"ACTIVE_URLS\", \"ACTIVE_SOURCES_INFO\", \"SOURCE_TYPES\", \"SOURCES\",\n\n    # Company raw info\n    \"COMPANY_RAW\", \"COMPANY_IS_STAFFING\",\n\n    # Raw or text-heavy fields\n    \"TITLE_RAW\", \"BODY\",\n\n    # Modeled / derived fields\n    \"MODELED_EXPIRED\", \"MODELED_DURATION\",\n\n    # Educational levels (redundant versions)\n    \"EDUCATION_LEVELS\", \"EDUCATION_LEVELS_NAME\",\n    \"MIN_EDULEVELS\", \"MIN_EDULEVELS_NAME\", \"MAX_EDULEVELS\",\n\n    # Redundant NAICS / SOC codes\n    \"NAICS_2022_2\", \"NAICS_2022_2_NAME\",\n    \"NAICS_2022_3\", \"NAICS_2022_3_NAME\",\n    \"SOC_2\", \"SOC_3\", \"SOC_5\"\n]\n\n# Drop columns, ignore if a column is missing\ndf1.drop(columns=columns_to_drop, inplace=True, errors=\"ignore\")\n\n# Display the first few rows to confirm\ndf1.head()\n\n\n\n\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nos.makedirs(\"figureswxw\", exist_ok=True)\n\n\n\n\nCode\nimport missingno as msno\nimport matplotlib.pyplot as plt\nmsno.heatmap(df1)\n\nplt.title(\"Missing Values Heatmap\")\nplt.tight_layout()\nplt.savefig(\"figureswxw/missing_values_heatmap.png\", dpi=300)\nplt.show()\n\n\n\n\n\n\n\n\n\nCode\n# Drop columns with &gt;50% missing values\ndf1.dropna(axis=1, thresh=len(df1) * 0.5, inplace=True)\n\n\nif \"SALARY\" in df1.columns:\n    df1[\"SALARY\"] = df1[\"SALARY\"].fillna(df1[\"SALARY\"].median())\n\n    df1[\"DURATION\"] = df1[\"DURATION\"].fillna(df1[\"DURATION\"].median())\n\ncategorical_columns = [\"REMOTE_TYPE_NAME\", \"COMPANY_NAME\", \"MAX_EDULEVELS_NAME\"]\n\nfor col in categorical_columns:\n    if col in df1.columns:\n        df1[col] = df1[col].fillna(\"Unknown\")\n\n\ndf1.info()\n\n\n\n\n\n\ndf1.drop_duplicates(subset=[\"TITLE_CLEAN\", \"COMPANY_NAME\", \"CITY_NAME\", \"POSTED\"], inplace=True)\n\ndf1[\"REMOTE_TYPE_NAME\"].value_counts(dropna=False)\ndf1[\"EMPLOYMENT_TYPE_NAME\"].value_counts(dropna=False)\n\n\n\nCode\n#improve\n\ndf1[\"EMPLOYMENT_TYPE_NAME\"] = df1[\"EMPLOYMENT_TYPE_NAME\"].replace({\n    \"Part-time (â‰¤ 32 hours)\": \"Part-time (≤ 32 hours)\",\n    \"Part-time / full-time\": \"Part-time / Full-time\"\n})\ndf1[\"EMPLOYMENT_TYPE_NAME\"] = df1[\"EMPLOYMENT_TYPE_NAME\"].fillna(\"Unknown\")\ndf1[\"EMPLOYMENT_TYPE_NAME\"].value_counts()"
  },
  {
    "objectID": "data_cleaning.html#lode-dataset",
    "href": "data_cleaning.html#lode-dataset",
    "title": "Data Cleaning",
    "section": "",
    "text": "/var/folders/4f/s815nnpx1_15rfdwxj6cwrtr0000gn/T/ipykernel_92605/3038307670.py:2: DtypeWarning: Columns (19,30) have mixed types. Specify dtype option on import or set low_memory=False.\n  df1 = pd.read_csv(\"./data/lightcast_job_postings.csv\")\n\n\n\ndf1.head()\ndf1.info()\ndf1.describe()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 72498 entries, 0 to 72497\nColumns: 131 entries, ID to NAICS_2022_6_NAME\ndtypes: float64(38), object(93)\nmemory usage: 72.5+ MB\n\n\n\n\n\n\n\n\n\nDUPLICATES\nDURATION\nMODELED_DURATION\nCOMPANY\nMIN_EDULEVELS\nMAX_EDULEVELS\nEMPLOYMENT_TYPE\nMIN_YEARS_EXPERIENCE\nMAX_YEARS_EXPERIENCE\nSALARY\n...\nLOT_OCCUPATION_GROUP\nLOT_V6_SPECIALIZED_OCCUPATION\nLOT_V6_OCCUPATION\nLOT_V6_OCCUPATION_GROUP\nLOT_V6_CAREER_AREA\nNAICS_2022_2\nNAICS_2022_3\nNAICS_2022_4\nNAICS_2022_5\nNAICS_2022_6\n\n\n\n\ncount\n72476.000000\n45182.000000\n53208.000000\n7.245400e+04\n72454.000000\n16315.000000\n72454.000000\n49352.000000\n8430.000000\n30808.000000\n...\n72454.000000\n7.245400e+04\n72454.000000\n72454.000000\n72454.000000\n72454.000000\n72454.000000\n72454.000000\n72454.000000\n72454.000000\n\n\nmean\n1.081627\n22.322695\n19.737615\n3.702704e+07\n31.482527\n2.833834\n1.058768\n5.486444\n3.773903\n117953.755031\n...\n2239.204475\n2.239318e+07\n223931.694096\n2239.204475\n22.281158\n58.352555\n587.864590\n5883.121995\n58834.317125\n588345.683937\n\n\nstd\n2.807512\n14.359085\n12.963769\n3.015089e+07\n44.747433\n0.584028\n0.286997\n3.322241\n2.576739\n45133.878359\n...\n285.424309\n2.854275e+06\n28542.747473\n285.424309\n2.854360\n18.626415\n186.259064\n1864.093904\n18642.971892\n186431.744508\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000e+00\n0.000000\n1.000000\n1.000000\n0.000000\n0.000000\n15860.000000\n...\n1111.000000\n1.111101e+07\n111110.000000\n1111.000000\n11.000000\n11.000000\n111.000000\n1111.000000\n11115.000000\n111150.000000\n\n\n25%\n0.000000\n11.000000\n10.000000\n6.505993e+06\n2.000000\n3.000000\n1.000000\n3.000000\n2.000000\n84928.500000\n...\n2310.000000\n2.310101e+07\n231010.000000\n2310.000000\n23.000000\n52.000000\n522.000000\n5223.000000\n52232.000000\n522320.000000\n\n\n50%\n0.000000\n18.000000\n16.000000\n3.761516e+07\n2.000000\n3.000000\n1.000000\n5.000000\n3.000000\n116300.000000\n...\n2311.000000\n2.311131e+07\n231113.000000\n2311.000000\n23.000000\n54.000000\n541.000000\n5415.000000\n54151.000000\n541519.000000\n\n\n75%\n1.000000\n32.000000\n28.000000\n4.330689e+07\n99.000000\n3.000000\n1.000000\n8.000000\n5.000000\n145600.000000\n...\n2311.000000\n2.311131e+07\n231113.000000\n2311.000000\n23.000000\n56.000000\n561.000000\n5614.000000\n56149.000000\n561499.000000\n\n\nmax\n100.000000\n59.000000\n59.000000\n1.082365e+08\n99.000000\n4.000000\n3.000000\n15.000000\n14.000000\n500000.000000\n...\n2712.000000\n2.712111e+07\n271211.000000\n2712.000000\n27.000000\n99.000000\n999.000000\n9999.000000\n99999.000000\n999999.000000\n\n\n\n\n8 rows × 38 columns"
  },
  {
    "objectID": "data_cleaning.html#check-columns-information",
    "href": "data_cleaning.html#check-columns-information",
    "title": "Data Cleaning",
    "section": "",
    "text": "Code\ndf1.columns.tolist()\nprint(df1.columns.tolist())"
  },
  {
    "objectID": "data_cleaning.html#dropping-unnecessary-columns",
    "href": "data_cleaning.html#dropping-unnecessary-columns",
    "title": "Data Cleaning",
    "section": "",
    "text": "# Define columns that are irrelevant or redundant for our analysis\ncolumns_to_drop = [\n    # Tracking and metadata\n    \"ID\", \"LAST_UPDATED_DATE\", \"LAST_UPDATED_TIMESTAMP\", \"DUPLICATES\",\n    \"URL\", \"ACTIVE_URLS\", \"ACTIVE_SOURCES_INFO\", \"SOURCE_TYPES\", \"SOURCES\",\n\n    # Company raw info\n    \"COMPANY_RAW\", \"COMPANY_IS_STAFFING\",\n\n    # Raw or text-heavy fields\n    \"TITLE_RAW\", \"BODY\",\n\n    # Modeled / derived fields\n    \"MODELED_EXPIRED\", \"MODELED_DURATION\",\n\n    # Educational levels (redundant versions)\n    \"EDUCATION_LEVELS\", \"EDUCATION_LEVELS_NAME\",\n    \"MIN_EDULEVELS\", \"MIN_EDULEVELS_NAME\", \"MAX_EDULEVELS\",\n\n    # Redundant NAICS / SOC codes\n    \"NAICS_2022_2\", \"NAICS_2022_2_NAME\",\n    \"NAICS_2022_3\", \"NAICS_2022_3_NAME\",\n    \"SOC_2\", \"SOC_3\", \"SOC_5\"\n]\n\n# Drop columns, ignore if a column is missing\ndf1.drop(columns=columns_to_drop, inplace=True, errors=\"ignore\")\n\n# Display the first few rows to confirm\ndf1.head()"
  },
  {
    "objectID": "data_cleaning.html#handling-missing-values",
    "href": "data_cleaning.html#handling-missing-values",
    "title": "Data Cleaning",
    "section": "",
    "text": "Code\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nos.makedirs(\"figureswxw\", exist_ok=True)\n\n\n\n\nCode\nimport missingno as msno\nimport matplotlib.pyplot as plt\nmsno.heatmap(df1)\n\nplt.title(\"Missing Values Heatmap\")\nplt.tight_layout()\nplt.savefig(\"figureswxw/missing_values_heatmap.png\", dpi=300)\nplt.show()\n\n\n\n\n\n\n\n\n\nCode\n# Drop columns with &gt;50% missing values\ndf1.dropna(axis=1, thresh=len(df1) * 0.5, inplace=True)\n\n\nif \"SALARY\" in df1.columns:\n    df1[\"SALARY\"] = df1[\"SALARY\"].fillna(df1[\"SALARY\"].median())\n\n    df1[\"DURATION\"] = df1[\"DURATION\"].fillna(df1[\"DURATION\"].median())\n\ncategorical_columns = [\"REMOTE_TYPE_NAME\", \"COMPANY_NAME\", \"MAX_EDULEVELS_NAME\"]\n\nfor col in categorical_columns:\n    if col in df1.columns:\n        df1[col] = df1[col].fillna(\"Unknown\")\n\n\ndf1.info()"
  },
  {
    "objectID": "data_cleaning.html#remove-duplicates",
    "href": "data_cleaning.html#remove-duplicates",
    "title": "Data Cleaning",
    "section": "",
    "text": "df1.drop_duplicates(subset=[\"TITLE_CLEAN\", \"COMPANY_NAME\", \"CITY_NAME\", \"POSTED\"], inplace=True)\n\ndf1[\"REMOTE_TYPE_NAME\"].value_counts(dropna=False)\ndf1[\"EMPLOYMENT_TYPE_NAME\"].value_counts(dropna=False)\n\n\n\nCode\n#improve\n\ndf1[\"EMPLOYMENT_TYPE_NAME\"] = df1[\"EMPLOYMENT_TYPE_NAME\"].replace({\n    \"Part-time (â‰¤ 32 hours)\": \"Part-time (≤ 32 hours)\",\n    \"Part-time / full-time\": \"Part-time / Full-time\"\n})\ndf1[\"EMPLOYMENT_TYPE_NAME\"] = df1[\"EMPLOYMENT_TYPE_NAME\"].fillna(\"Unknown\")\ndf1[\"EMPLOYMENT_TYPE_NAME\"].value_counts()"
  },
  {
    "objectID": "data_cleaning.html#remote-type-distribution",
    "href": "data_cleaning.html#remote-type-distribution",
    "title": "Data Cleaning",
    "section": "2.1 Remote Type distribution",
    "text": "2.1 Remote Type distribution\n\n\nCode\nremote_counts = df1[\"REMOTE_TYPE_NAME\"].value_counts()\n\nplt.figure(figsize=(10,6))\nsns.barplot(\n    x=remote_counts.index, \n    y=remote_counts.values, \n    palette=\"Set2\"\n)\nplt.title(\"Remote Type Distribution\")\nplt.ylabel(\"Number of Job Postings\")\nplt.xlabel(\"Remote Type\")\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.savefig(\"figureswxw/remote_type_distribution.png\", dpi=300)\nplt.show()\n\n\n\n\n\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf1[\"IS_AI\"] = df1[\"NAICS_2022_6_NAME\"].fillna(\"\").str.contains(\"AI|Artificial Intelligence\", case=False) | \\\n               df1[\"LOT_OCCUPATION\"].fillna(\"\").str.contains(\"AI|Artificial Intelligence\", case=False)\n\ndf1[\"IS_AI\"] = df1[\"IS_AI\"].map({True: \"AI\", False: \"Non-AI\"})"
  },
  {
    "objectID": "data_cleaning.html#top-10-states-by-job-postings",
    "href": "data_cleaning.html#top-10-states-by-job-postings",
    "title": "Data Cleaning",
    "section": "2.2 Top 10 states by job postings",
    "text": "2.2 Top 10 states by job postings\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf1 = pd.read_csv(\"data/lightcast_job_postings.csv\")\n\ncity_counts = df1[\"CITY_NAME\"].dropna().value_counts().head(10)\n\nplt.figure(figsize=(10,5))\nsns.barplot(x=city_counts.index, y=city_counts.values, palette=\"Accent\")\nplt.title(\"Top 10 Cities by Job Postings\")\nplt.ylabel(\"Number of Job Postings\")\nplt.xlabel(\"City\")\nplt.xticks(rotation=30)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.savefig(\"figureswxw/top_cities_job_postings.png\", dpi=300)\nplt.show()\n\n\n/var/folders/4f/s815nnpx1_15rfdwxj6cwrtr0000gn/T/ipykernel_8078/1316398421.py:6: DtypeWarning: Columns (19,30) have mixed types. Specify dtype option on import or set low_memory=False.\n  df1 = pd.read_csv(\"data/lightcast_job_postings.csv\")\n/var/folders/4f/s815nnpx1_15rfdwxj6cwrtr0000gn/T/ipykernel_8078/1316398421.py:11: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x=city_counts.index, y=city_counts.values, palette=\"Accent\")\n\n\n\n\n\nRemote type distribution"
  },
  {
    "objectID": "data_cleaning.html#top-10-cities-by-job-postings",
    "href": "data_cleaning.html#top-10-cities-by-job-postings",
    "title": "Data Cleaning",
    "section": "2.3 Top 10 cities by job postings",
    "text": "2.3 Top 10 cities by job postings\n\n\nCode\ncity_counts = df1[\"CITY_NAME\"].value_counts().head(10)\nplt.figure(figsize=(10,5))\nsns.barplot(x=city_counts.index, y=city_counts.values, palette=\"Accent\")\nplt.title(\"Top 10 Cities by Job Postings\")\nplt.ylabel(\"Number of Job Postings\")\nplt.xlabel(\"City\")\nplt.xticks(rotation=30)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.savefig(\"figureswxw/top_cities_job_postings.png\", dpi=300)\nplt.show()"
  },
  {
    "objectID": "data_cleaning.html#remote-type-by-top-5-states",
    "href": "data_cleaning.html#remote-type-by-top-5-states",
    "title": "Data Cleaning",
    "section": "2.4 Remote Type by Top 5 States",
    "text": "2.4 Remote Type by Top 5 States\n\n\nCode\ntop_states = df1[\"STATE_NAME\"].value_counts().head(5).index\ndf_top = df1[df1[\"STATE_NAME\"].isin(top_states)]\n\n\npivot = df_top.groupby([\"STATE_NAME\", \"REMOTE_TYPE_NAME\"]).size().unstack(fill_value=0)\n\nplt.figure(figsize=(10,6))\nsns.heatmap(pivot, annot=True, fmt=\"d\", cmap=\"Blues\")\nplt.title(\"Heatmap: Remote Type Count in Top 5 States\")\nplt.tight_layout()\nplt.savefig(\"figureswxw/heatmap_remote_type_top_states.png\", dpi=300)\nplt.show()"
  },
  {
    "objectID": "data_cleaning.html#industry-distribution-by-state",
    "href": "data_cleaning.html#industry-distribution-by-state",
    "title": "Data Cleaning",
    "section": "2.5 Industry Distribution by State",
    "text": "2.5 Industry Distribution by State\n\n\nCode\ntop_industries = df_top[\"NAICS_2022_6_NAME\"].value_counts().head(6).index\ndf_top[\"NAICS_GROUPED\"] = df_top[\"NAICS_2022_6_NAME\"].where(df_top[\"NAICS_2022_6_NAME\"].isin(top_industries), \"Other\")\n\n\npivot_industry = df_top.groupby([\"STATE_NAME\", \"NAICS_GROUPED\"]).size().unstack(fill_value=0)\n\npivot_industry.plot(kind=\"bar\", stacked=True, figsize=(12,8), colormap=\"Set1\")\nplt.title(\"Job Postings by Major Industry in Top 5 States\")\nplt.ylabel(\"Number of Job Postings\")\nplt.xlabel(\"State\")\nplt.xticks(rotation=0)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\nplt.legend(\n    title=\"NAICS_GROUPED\",\n    fontsize=10,\n    title_fontsize=10,\n    loc='upper center',\n    bbox_to_anchor=(0.5, -0.15),  \n    ncol=2,  \n    frameon=False  \n)\nplt.tight_layout()\nplt.savefig(\"figureswxw/industry_by_state_top_major.png\", dpi=300)\nplt.show()"
  },
  {
    "objectID": "data_cleaning.html#time-trend-of-remote-work-types",
    "href": "data_cleaning.html#time-trend-of-remote-work-types",
    "title": "Data Cleaning",
    "section": "2.4 Time Trend of Remote Work Types",
    "text": "2.4 Time Trend of Remote Work Types\n\n\nCode\nif \"POSTED\" in df1.columns:\n    df1[\"POSTED_DATE\"] = pd.to_datetime(df1[\"POSTED\"], errors='coerce')\n    df1 = df1.dropna(subset=[\"POSTED_DATE\"])\n    df1[\"POSTED_MONTH\"] = df1[\"POSTED_DATE\"].dt.to_period(\"M\")\n    \n    trend = df1.groupby([\"POSTED_MONTH\", \"REMOTE_TYPE_NAME\"]).size().unstack(fill_value=0)\n    \n    trend.plot(figsize=(14,7))\n    plt.title(\"Remote Work Trends Over Time\", fontsize=14)\n    plt.ylabel(\"Number of Job Postings\")\n    plt.xlabel(\"Month\")\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.savefig(\"figureswxw/remote_trend_over_time.png\", dpi=300)\n    plt.show()\nelse:\n    print(\"POSTED column not found in dataset.\")"
  },
  {
    "objectID": "data_cleaning.html#load-dataset",
    "href": "data_cleaning.html#load-dataset",
    "title": "Data Cleaning",
    "section": "",
    "text": "Code\nimport pandas as pd\ndf1 = pd.read_csv(\"./data/lightcast_job_postings.csv\")\n\n\n\n\nCode\ndf1.head()\ndf1.info()\ndf1.describe()"
  },
  {
    "objectID": "data_cleaning.html#top-10-states-ai-vs-non-ai-job-postings",
    "href": "data_cleaning.html#top-10-states-ai-vs-non-ai-job-postings",
    "title": "Data Cleaning",
    "section": "2.2 Top 10 states : AI vs Non-AI Job Postings",
    "text": "2.2 Top 10 states : AI vs Non-AI Job Postings\n\n\nCode\ntop_states = df1[\"STATE_NAME\"].value_counts().head(10).index\ndf_top_states = df1[df1[\"STATE_NAME\"].isin(top_states)]\n\npivot_states = df_top_states.groupby([\"STATE_NAME\", \"IS_AI\"]).size().unstack(fill_value=0)\n\npivot_states.plot(kind=\"bar\", stacked=True, figsize=(12,6), colormap=\"Set3\")\nplt.title(\"Top 10 States: AI vs Non-AI Job Postings\")\nplt.ylabel(\"Number of Job Postings\")\nplt.xlabel(\"State\")\nplt.xticks(rotation=30)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.savefig(\"figureswxw/top_states_ai_nonai.png\", dpi=300)\nplt.show()"
  },
  {
    "objectID": "data_cleaning.html#top-10-cities-ai-vs-non-ai-job-postings",
    "href": "data_cleaning.html#top-10-cities-ai-vs-non-ai-job-postings",
    "title": "Data Cleaning",
    "section": "2.3 Top 10 cities: AI vs Non-AI Job Postings",
    "text": "2.3 Top 10 cities: AI vs Non-AI Job Postings\n\n\nCode\ntop_cities = df1[\"CITY_NAME\"].value_counts().head(10).index\ndf_top_cities = df1[df1[\"CITY_NAME\"].isin(top_cities)]\n\npivot_cities = df_top_cities.groupby([\"CITY_NAME\", \"IS_AI\"]).size().unstack(fill_value=0)\n\npivot_cities.plot(kind=\"bar\", stacked=True, figsize=(12,6), colormap=\"Set1\")\nplt.title(\"Top 10 Cities: AI vs Non-AI Job Postings\")\nplt.ylabel(\"Number of Job Postings\")\nplt.xlabel(\"City\")\nplt.xticks(rotation=30)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.savefig(\"figureswxw/top_cities_ai_nonai.png\", dpi=300)\nplt.show()"
  },
  {
    "objectID": "data_cleaning.html#tech-hubs-vs-other-locations-hiring-trends",
    "href": "data_cleaning.html#tech-hubs-vs-other-locations-hiring-trends",
    "title": "Data Cleaning",
    "section": "2.5 Tech Hubs vs Other Locations Hiring Trends",
    "text": "2.5 Tech Hubs vs Other Locations Hiring Trends\n\n\nCode\ndf1[\"IS_HUB\"] = df1[\"CITY_NAME\"].apply(lambda x: \"Hub\" if x in [\"San Francisco\", \"Austin\", \"Boston\"] else \"Other\")\n\npivot_hub = df1.groupby([\"POSTED_MONTH\", \"IS_HUB\"]).size().unstack(fill_value=0)\n\npivot_hub.plot(figsize=(14,7))\nplt.title(\"Hiring Trends: Tech Hubs vs Other Locations\")\nplt.ylabel(\"Number of Job Postings\")\nplt.xlabel(\"Month\")\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.savefig(\"figureswxw/techhub_vs_other_trend.png\", dpi=300)\nplt.show()"
  },
  {
    "objectID": "data_cleaning.html#remote-job-trend-by-industry",
    "href": "data_cleaning.html#remote-job-trend-by-industry",
    "title": "Data Cleaning",
    "section": "2.6 Remote Job Trend by Industry",
    "text": "2.6 Remote Job Trend by Industry\n\n\nCode\ntop_industries = (\n    df1.groupby(\"NAICS_2022_6_NAME\").size()\n    .sort_values(ascending=False)\n    .head(10)\n    .index\n)\n\n\ndf_top_ind = df1[df1[\"NAICS_2022_6_NAME\"].isin(top_industries)]\n\n\ndf_top_ind[\"POSTED_DATE\"] = pd.to_datetime(df_top_ind[\"POSTED\"], errors='coerce')\ndf_top_ind = df_top_ind.dropna(subset=[\"POSTED_DATE\"])\ndf_top_ind[\"POSTED_MONTH\"] = df_top_ind[\"POSTED_DATE\"].dt.to_period(\"M\")\n\n\npivot = df_top_ind.groupby([\"POSTED_MONTH\", \"NAICS_2022_6_NAME\"]).size().unstack(fill_value=0)\n\npivot.plot(figsize=(14,7))\nplt.title(\"Remote Job Trends by Top 5 Industries Over Time\", fontsize=14)\nplt.ylabel(\"Number of Job Postings\")\nplt.xlabel(\"Month\")\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.legend(\n    title=\"NAICS_2022_6_NAME\",\n    loc='upper center',\n    bbox_to_anchor=(0.5, -0.15),  \n    ncol=2,                       \n    frameon=False\n)\nplt.tight_layout()\nplt.savefig(\"figureswxw/remote_trend_top5_industry.png\", dpi=300)\nplt.show()"
  },
  {
    "objectID": "data_cleaning.html#urbanrural-region-ai-vs-non-ai-job-postings",
    "href": "data_cleaning.html#urbanrural-region-ai-vs-non-ai-job-postings",
    "title": "Data Cleaning",
    "section": "2.7 Urban/Rural Region: AI vs Non-AI Job Postings",
    "text": "2.7 Urban/Rural Region: AI vs Non-AI Job Postings\n\n\nCode\nurban_cities = [\n    \"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"San Francisco\",\n    \"Austin\", \"Boston\", \"Dallas\", \"Seattle\", \"Washington\", \"Atlanta\"\n]\n\ndf1[\"CITY_NAME_CLEAN\"] = df1[\"CITY_NAME\"].str.split(\",\").str[0].str.strip().str.title()\n\ndf1[\"Urban_Rural\"] = df1[\"CITY_NAME_CLEAN\"].apply(\n    lambda x: \"Urban\" if x in urban_cities else \"Rural\"\n)\n\nprint(df1[\"Urban_Rural\"].value_counts())\n\n\n\n\nCode\nrequired_columns = [\"Urban_Rural\", \"IS_AI\"]\nmissing_columns = [col for col in required_columns if col not in df1.columns]\n\nif not missing_columns:\n\n    pivot_urban = df1.groupby([\"Urban_Rural\", \"IS_AI\"]).size().unstack(fill_value=0)\n\n    for region in pivot_urban.index:\n        data = pivot_urban.loc[region]\n        plt.figure(figsize=(6,6))\n        plt.pie(\n            data,\n            labels=data.index,\n            autopct='%1.1f%%',\n            startangle=90,\n            colors=[\"#ff9999\",\"#66b3ff\"],\n            wedgeprops={'edgecolor': 'black'}\n        )\n        plt.title(f\"{region} - AI vs Non-AI Job Postings\")\n        plt.tight_layout()\n        fname = f\"figureswxw/{region.lower()}_ai_nonai_pie.png\"\n        plt.savefig(fname, dpi=300)\n        plt.show()\n        print(f\"Chart saved as {fname}\")\nelse:\n    print(f\"The following required columns are missing: {missing_columns}. Please check your dataset.\")"
  },
  {
    "objectID": "data_cleaning_eda.html",
    "href": "data_cleaning_eda.html",
    "title": "Data Cleaning & EDA",
    "section": "",
    "text": "This page provides an overview of the data cleaning and initial exploration process for the job market dataset.\nContent will be added here.\n\n\n\n# Define columns that are irrelevant or redundant for our analysis\ncolumns_to_drop = [\n    # Tracking and metadata\n    \"ID\", \"LAST_UPDATED_DATE\", \"LAST_UPDATED_TIMESTAMP\", \"DUPLICATES\",\n    \"URL\", \"ACTIVE_URLS\", \"ACTIVE_SOURCES_INFO\", \"SOURCE_TYPES\", \"SOURCES\",\n\n    # Company raw info\n    \"COMPANY_RAW\", \"COMPANY_IS_STAFFING\",\n\n    # Raw or text-heavy fields\n    \"TITLE_RAW\", \"BODY\",\n\n    # Modeled / derived fields\n    \"MODELED_EXPIRED\", \"MODELED_DURATION\",\n\n    # Educational levels (redundant versions)\n    \"EDUCATION_LEVELS\", \"EDUCATION_LEVELS_NAME\",\n    \"MIN_EDULEVELS\", \"MIN_EDULEVELS_NAME\", \"MAX_EDULEVELS\",\n\n    # Redundant NAICS / SOC codes\n    \"NAICS_2022_2\", \"NAICS_2022_2_NAME\",\n    \"NAICS_2022_3\", \"NAICS_2022_3_NAME\",\n    \"SOC_2\", \"SOC_3\", \"SOC_5\"\n]\n\n# Drop columns, ignore if a column is missing\ndf1.drop(columns=columns_to_drop, inplace=True, errors=\"ignore\")\n\n# Display the first few rows to confirm\ndf1.head()\n\n\n\n\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nos.makedirs(\"figureswxw\", exist_ok=True)\n\n\n\n\nCode\nimport missingno as msno\nimport matplotlib.pyplot as plt\nmsno.heatmap(df1)\n\nplt.title(\"Missing Values Heatmap\")\nplt.tight_layout()\nplt.savefig(\"figureswxw/missing_values_heatmap.png\", dpi=300,  bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\nCode\n# Drop columns with &gt;50% missing values\ndf1.dropna(axis=1, thresh=len(df1) * 0.5, inplace=True)\n\n\nif \"SALARY\" in df1.columns:\n    df1[\"SALARY\"] = df1[\"SALARY\"].fillna(df1[\"SALARY\"].median())\n\n    df1[\"DURATION\"] = df1[\"DURATION\"].fillna(df1[\"DURATION\"].median())\n\ncategorical_columns = [\"REMOTE_TYPE_NAME\", \"COMPANY_NAME\", \"MAX_EDULEVELS_NAME\"]\n\nfor col in categorical_columns:\n    if col in df1.columns:\n        df1[col] = df1[col].fillna(\"Unknown\")\n\n\ndf1.info()\n\n\n\n\n\n\ndf1.drop_duplicates(subset=[\"TITLE_CLEAN\", \"COMPANY_NAME\", \"CITY_NAME\", \"POSTED\"], inplace=True)\n\ndf1[\"REMOTE_TYPE_NAME\"].value_counts(dropna=False)\ndf1[\"EMPLOYMENT_TYPE_NAME\"].value_counts(dropna=False)\n\n\n\nCode\n#improve\n\ndf1[\"EMPLOYMENT_TYPE_NAME\"] = df1[\"EMPLOYMENT_TYPE_NAME\"].replace({\n    \"Part-time (â‰¤ 32 hours)\": \"Part-time (≤ 32 hours)\",\n    \"Part-time / full-time\": \"Part-time / Full-time\"\n})\ndf1[\"EMPLOYMENT_TYPE_NAME\"] = df1[\"EMPLOYMENT_TYPE_NAME\"].fillna(\"Unknown\")\ndf1[\"EMPLOYMENT_TYPE_NAME\"].value_counts()"
  },
  {
    "objectID": "data_cleaning_eda.html#load-dataset",
    "href": "data_cleaning_eda.html#load-dataset",
    "title": "Data Cleaning",
    "section": "",
    "text": "Code\nimport pandas as pd\ndf1 = pd.read_csv(\"./data/lightcast_job_postings.csv\")\n\n\n\n\nCode\ndf1.head()\ndf1.info()\ndf1.describe()"
  },
  {
    "objectID": "data_cleaning_eda.html#check-columns-information",
    "href": "data_cleaning_eda.html#check-columns-information",
    "title": "Data Cleaning",
    "section": "",
    "text": "Code\ndf1.columns.tolist()\nprint(df1.columns.tolist())"
  },
  {
    "objectID": "data_cleaning_eda.html#dropping-unnecessary-columns",
    "href": "data_cleaning_eda.html#dropping-unnecessary-columns",
    "title": "Data Cleaning & EDA",
    "section": "",
    "text": "# Define columns that are irrelevant or redundant for our analysis\ncolumns_to_drop = [\n    # Tracking and metadata\n    \"ID\", \"LAST_UPDATED_DATE\", \"LAST_UPDATED_TIMESTAMP\", \"DUPLICATES\",\n    \"URL\", \"ACTIVE_URLS\", \"ACTIVE_SOURCES_INFO\", \"SOURCE_TYPES\", \"SOURCES\",\n\n    # Company raw info\n    \"COMPANY_RAW\", \"COMPANY_IS_STAFFING\",\n\n    # Raw or text-heavy fields\n    \"TITLE_RAW\", \"BODY\",\n\n    # Modeled / derived fields\n    \"MODELED_EXPIRED\", \"MODELED_DURATION\",\n\n    # Educational levels (redundant versions)\n    \"EDUCATION_LEVELS\", \"EDUCATION_LEVELS_NAME\",\n    \"MIN_EDULEVELS\", \"MIN_EDULEVELS_NAME\", \"MAX_EDULEVELS\",\n\n    # Redundant NAICS / SOC codes\n    \"NAICS_2022_2\", \"NAICS_2022_2_NAME\",\n    \"NAICS_2022_3\", \"NAICS_2022_3_NAME\",\n    \"SOC_2\", \"SOC_3\", \"SOC_5\"\n]\n\n# Drop columns, ignore if a column is missing\ndf1.drop(columns=columns_to_drop, inplace=True, errors=\"ignore\")\n\n# Display the first few rows to confirm\ndf1.head()"
  },
  {
    "objectID": "data_cleaning_eda.html#handling-missing-values",
    "href": "data_cleaning_eda.html#handling-missing-values",
    "title": "Data Cleaning & EDA",
    "section": "",
    "text": "Code\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nos.makedirs(\"figureswxw\", exist_ok=True)\n\n\n\n\nCode\nimport missingno as msno\nimport matplotlib.pyplot as plt\nmsno.heatmap(df1)\n\nplt.title(\"Missing Values Heatmap\")\nplt.tight_layout()\nplt.savefig(\"figureswxw/missing_values_heatmap.png\", dpi=300,  bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\nCode\n# Drop columns with &gt;50% missing values\ndf1.dropna(axis=1, thresh=len(df1) * 0.5, inplace=True)\n\n\nif \"SALARY\" in df1.columns:\n    df1[\"SALARY\"] = df1[\"SALARY\"].fillna(df1[\"SALARY\"].median())\n\n    df1[\"DURATION\"] = df1[\"DURATION\"].fillna(df1[\"DURATION\"].median())\n\ncategorical_columns = [\"REMOTE_TYPE_NAME\", \"COMPANY_NAME\", \"MAX_EDULEVELS_NAME\"]\n\nfor col in categorical_columns:\n    if col in df1.columns:\n        df1[col] = df1[col].fillna(\"Unknown\")\n\n\ndf1.info()"
  },
  {
    "objectID": "data_cleaning_eda.html#remove-duplicates",
    "href": "data_cleaning_eda.html#remove-duplicates",
    "title": "Data Cleaning & EDA",
    "section": "",
    "text": "df1.drop_duplicates(subset=[\"TITLE_CLEAN\", \"COMPANY_NAME\", \"CITY_NAME\", \"POSTED\"], inplace=True)\n\ndf1[\"REMOTE_TYPE_NAME\"].value_counts(dropna=False)\ndf1[\"EMPLOYMENT_TYPE_NAME\"].value_counts(dropna=False)\n\n\n\nCode\n#improve\n\ndf1[\"EMPLOYMENT_TYPE_NAME\"] = df1[\"EMPLOYMENT_TYPE_NAME\"].replace({\n    \"Part-time (â‰¤ 32 hours)\": \"Part-time (≤ 32 hours)\",\n    \"Part-time / full-time\": \"Part-time / Full-time\"\n})\ndf1[\"EMPLOYMENT_TYPE_NAME\"] = df1[\"EMPLOYMENT_TYPE_NAME\"].fillna(\"Unknown\")\ndf1[\"EMPLOYMENT_TYPE_NAME\"].value_counts()"
  },
  {
    "objectID": "data_cleaning_eda.html#remote-type-distribution",
    "href": "data_cleaning_eda.html#remote-type-distribution",
    "title": "Data Cleaning & EDA",
    "section": "2.1 Remote Type distribution",
    "text": "2.1 Remote Type distribution\n\n\nCode\nremote_counts = df1[\"REMOTE_TYPE_NAME\"].value_counts()\n\nplt.figure(figsize=(10,6))\nsns.barplot(\n    x=remote_counts.index, \n    y=remote_counts.values, \n    palette=\"Set2\"\n)\nplt.title(\"Remote Type Distribution\")\nplt.ylabel(\"Number of Job Postings\")\nplt.xlabel(\"Remote Type\")\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.savefig(\"figureswxw/remote_type_distribution.png\", dpi=300,  bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf1[\"IS_AI\"] = df1[\"NAICS_2022_6_NAME\"].fillna(\"\").str.contains(\"AI|Artificial Intelligence\", case=False) | \\\n               df1[\"LOT_OCCUPATION\"].fillna(\"\").str.contains(\"AI|Artificial Intelligence\", case=False)\n\ndf1[\"IS_AI\"] = df1[\"IS_AI\"].map({True: \"AI\", False: \"Non-AI\"})"
  },
  {
    "objectID": "data_cleaning_eda.html#top-10-states-ai-vs-non-ai-job-postings",
    "href": "data_cleaning_eda.html#top-10-states-ai-vs-non-ai-job-postings",
    "title": "Data Cleaning & EDA",
    "section": "2.2 Top 10 states : AI vs Non-AI Job Postings",
    "text": "2.2 Top 10 states : AI vs Non-AI Job Postings\n\n\nCode\ntop_states = df1[\"STATE_NAME\"].value_counts().head(10).index\ndf_top_states = df1[df1[\"STATE_NAME\"].isin(top_states)]\n\npivot_states = df_top_states.groupby([\"STATE_NAME\", \"IS_AI\"]).size().unstack(fill_value=0)\n\npivot_states.plot(kind=\"bar\", stacked=True, figsize=(12,6), colormap=\"Set3\")\nplt.title(\"Top 10 States: AI vs Non-AI Job Postings\")\nplt.ylabel(\"Number of Job Postings\")\nplt.xlabel(\"State\")\nplt.xticks(rotation=30)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.savefig(\"figureswxw/top_states_ai_nonai.png\", dpi=300,  bbox_inches='tight')\nplt.show()"
  },
  {
    "objectID": "data_cleaning_eda.html#top-10-cities-ai-vs-non-ai-job-postings",
    "href": "data_cleaning_eda.html#top-10-cities-ai-vs-non-ai-job-postings",
    "title": "Data Cleaning & EDA",
    "section": "2.3 Top 10 cities: AI vs Non-AI Job Postings",
    "text": "2.3 Top 10 cities: AI vs Non-AI Job Postings\n\n\nCode\ntop_cities = df1[\"CITY_NAME\"].value_counts().head(10).index\ndf_top_cities = df1[df1[\"CITY_NAME\"].isin(top_cities)]\n\npivot_cities = df_top_cities.groupby([\"CITY_NAME\", \"IS_AI\"]).size().unstack(fill_value=0)\n\npivot_cities.plot(kind=\"bar\", stacked=True, figsize=(12,6), colormap=\"Set1\")\nplt.title(\"Top 10 Cities: AI vs Non-AI Job Postings\")\nplt.ylabel(\"Number of Job Postings\")\nplt.xlabel(\"City\")\nplt.xticks(rotation=30)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.savefig(\"figureswxw/top_cities_ai_nonai.png\", dpi=300)\nplt.show()"
  },
  {
    "objectID": "data_cleaning_eda.html#time-trend-of-remote-work-types",
    "href": "data_cleaning_eda.html#time-trend-of-remote-work-types",
    "title": "Data Cleaning & EDA",
    "section": "2.4 Time Trend of Remote Work Types",
    "text": "2.4 Time Trend of Remote Work Types\n\n\nCode\nif \"POSTED\" in df1.columns:\n    df1[\"POSTED_DATE\"] = pd.to_datetime(df1[\"POSTED\"], errors='coerce')\n    df1 = df1.dropna(subset=[\"POSTED_DATE\"])\n    df1[\"POSTED_MONTH\"] = df1[\"POSTED_DATE\"].dt.to_period(\"M\")\n    \n    trend = df1.groupby([\"POSTED_MONTH\", \"REMOTE_TYPE_NAME\"]).size().unstack(fill_value=0)\n    \n    trend.plot(figsize=(14,7))\n    plt.title(\"Remote Work Trends Over Time\", fontsize=14)\n    plt.ylabel(\"Number of Job Postings\")\n    plt.xlabel(\"Month\")\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.savefig(\"figureswxw/remote_trend_over_time.png\", dpi=300,  bbox_inches='tight')\n    plt.show()\nelse:\n    print(\"POSTED column not found in dataset.\")"
  },
  {
    "objectID": "data_cleaning_eda.html#tech-hubs-vs-other-locations-hiring-trends",
    "href": "data_cleaning_eda.html#tech-hubs-vs-other-locations-hiring-trends",
    "title": "Data Cleaning & EDA",
    "section": "2.5 Tech Hubs vs Other Locations Hiring Trends",
    "text": "2.5 Tech Hubs vs Other Locations Hiring Trends\n\n\nCode\ndf1[\"IS_HUB\"] = df1[\"CITY_NAME\"].apply(lambda x: \"Hub\" if x in [\"San Francisco\", \"Austin\", \"Boston\"] else \"Other\")\n\npivot_hub = df1.groupby([\"POSTED_MONTH\", \"IS_HUB\"]).size().unstack(fill_value=0)\n\npivot_hub.plot(figsize=(14,7))\nplt.title(\"Hiring Trends: Tech Hubs vs Other Locations\")\nplt.ylabel(\"Number of Job Postings\")\nplt.xlabel(\"Month\")\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.savefig(\"figureswxw/techhub_vs_other_trend.png\", dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
    "objectID": "data_cleaning_eda.html#remote-job-trend-by-industry",
    "href": "data_cleaning_eda.html#remote-job-trend-by-industry",
    "title": "Data Cleaning & EDA",
    "section": "2.6 Remote Job Trend by Industry",
    "text": "2.6 Remote Job Trend by Industry\n\n\nCode\ntop_industries = (\n    df1.groupby(\"NAICS_2022_6_NAME\").size()\n    .sort_values(ascending=False)\n    .head(10)\n    .index\n)\n\n\ndf_top_ind = df1[df1[\"NAICS_2022_6_NAME\"].isin(top_industries)]\n\n\ndf_top_ind[\"POSTED_DATE\"] = pd.to_datetime(df_top_ind[\"POSTED\"], errors='coerce')\ndf_top_ind = df_top_ind.dropna(subset=[\"POSTED_DATE\"])\ndf_top_ind[\"POSTED_MONTH\"] = df_top_ind[\"POSTED_DATE\"].dt.to_period(\"M\")\n\n\npivot = df_top_ind.groupby([\"POSTED_MONTH\", \"NAICS_2022_6_NAME\"]).size().unstack(fill_value=0)\n\npivot.plot(figsize=(14,7))\nplt.title(\"Remote Job Trends by Top 5 Industries Over Time\", fontsize=14)\nplt.ylabel(\"Number of Job Postings\")\nplt.xlabel(\"Month\")\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.legend(\n    title=\"NAICS_2022_6_NAME\",\n    loc='upper center',\n    bbox_to_anchor=(0.5, -0.15),  \n    ncol=2,                       \n    frameon=False\n)\nplt.tight_layout()\nplt.savefig(\"figureswxw/remote_trend_top5_industry.png\", dpi=30,  bbox_inches='tight')\nplt.show()"
  },
  {
    "objectID": "data_cleaning_eda.html#urbanrural-region-ai-vs-non-ai-job-postings",
    "href": "data_cleaning_eda.html#urbanrural-region-ai-vs-non-ai-job-postings",
    "title": "Data Cleaning & EDA",
    "section": "2.7 Urban/Rural Region: AI vs Non-AI Job Postings",
    "text": "2.7 Urban/Rural Region: AI vs Non-AI Job Postings\n\n\nCode\nurban_cities = [\n    \"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"San Francisco\",\n    \"Austin\", \"Boston\", \"Dallas\", \"Seattle\", \"Washington\", \"Atlanta\"\n]\n\ndf1[\"CITY_NAME_CLEAN\"] = df1[\"CITY_NAME\"].str.split(\",\").str[0].str.strip().str.title()\n\ndf1[\"Urban_Rural\"] = df1[\"CITY_NAME_CLEAN\"].apply(\n    lambda x: \"Urban\" if x in urban_cities else \"Rural\"\n)\n\nprint(df1[\"Urban_Rural\"].value_counts())\n\n\n\n2.7.1 Stacked Bar Chart\n\n\nCode\nif {\"Urban_Rural\", \"IS_AI\"}.issubset(df1.columns):\n    pivot_urban = df1.groupby([\"Urban_Rural\", \"IS_AI\"]).size().unstack(fill_value=0)\n\n    ax = pivot_urban.plot(\n        kind=\"bar\",\n        stacked=True,\n        figsize=(8, 5),\n        color=[\"#ff9999\", \"#66b3ff\"],\n        edgecolor=\"black\"\n    )\n    ax.set_title(\"Urban and Rural Regions: AI vs Non-AI Job Postings\", fontsize=14)\n    ax.set_ylabel(\"Number of Job Postings\")\n    ax.set_xlabel(\"Region Type\")\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n    ax.grid(axis='y', linestyle='--', alpha=0.7)\n\n    plt.tight_layout()\n    plt.savefig(\"figureswxw/urban_rural_ai_nonai_bar.png\", dpi=300,  bbox_inches='tight')\n    plt.show()\n\nelse:\n    print(\"Required columns are missing. Please check your dataset.\")\n\n\n\n\n\n\n\n\n\n2.7.2 Pie Chart\n\n\nCode\nimport matplotlib.pyplot as plt\n\nif {\"Urban_Rural\", \"IS_AI\"}.issubset(df1.columns):\n    pivot_urban = df1.groupby([\"Urban_Rural\", \"IS_AI\"]).size().unstack(fill_value=0)\n\n    fig, axes = plt.subplots(1, 2, figsize=(8, 4))  \n\n    for ax, region in zip(axes, [\"Urban\", \"Rural\"]):\n        data = pivot_urban.loc[region]\n        ax.pie(\n            data,\n            labels=data.index,\n            autopct='%1.1f%%',\n            startangle=90,\n            colors=[\"#ff9999\", \"#66b3ff\"],\n            wedgeprops={'edgecolor': 'black'}\n        )\n        ax.set_title(f\"{region} - AI vs Non-AI\")\n\n    plt.tight_layout()\n    plt.savefig(\"figureswxw/urban_rural_ai_nonai_pie_combined.png\", dpi=300,  bbox_inches='tight')\n    plt.show()\n\nelse:\n    print(\"Required columns are missing. Please check your dataset.\")"
  },
  {
    "objectID": "eda_enhance.html",
    "href": "eda_enhance.html",
    "title": "Extended EDA: Geographic Distribution",
    "section": "",
    "text": "This section presents an enhanced exploratory data analysis of job postings in the United States, with particular attention to remote work types and geographic patterns across industries, states, and cities. The objective is to examine how the distribution of remote, hybrid, and onsite jobs differs across regions, and how industry-specific trends reflect broader labor market shifts. Through this analysis, we aim to capture regional disparities, identify emerging hiring hubs, and understand how remote work adoption is reshaping spatial dynamics in the 2024 job market.\n\n1 Job Distribution by Industry (NAICS Level 2)\n\n\nCode\nwrapped_labels = ['\\n'.join(textwrap.wrap(label, width=10)) for label in industry_counts[\"Industry\"]]\n\nplt.figure(figsize=(18, 12))\nax = sns.barplot(data=industry_counts, x=\"Industry\", y=\"Job_Count\", width=0.8)\n\nax.set_xticklabels(wrapped_labels, rotation=0, ha='center')\n\nplt.title(\"Job Count by Industry (NAICS Level 2)\")\nplt.xticks(rotation=45, fontsize=8)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.savefig(\"figurestyj/job_count_by_industry.png\", dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\n2 Job Count by City (Top 15)\n\n\nCode\ncity_counts = df[\"CITY_NAME\"].value_counts().head(15).reset_index()\ncity_counts.columns = [\"City\", \"Job_Count\"]\n\nplt.figure(figsize=(12, 6))\nsns.barplot(data=city_counts, x=\"City\", y=\"Job_Count\", palette=\"Accent\")\nplt.title(\"Top 15 Cities by Job Count\")\nplt.xticks(rotation=45, ha=\"right\")\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.savefig(\"figurestyj/top_cities_job_count.png\", dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\n3 Job Count by State\n\n\nCode\nstate_counts = df[\"STATE_NAME\"].value_counts().reset_index()\nstate_counts.columns = [\"State\", \"Job_Count\"]\n\nplt.figure(figsize=(12, 6))\nsns.barplot(data=state_counts.head(15), x=\"State\", y=\"Job_Count\", palette=\"Set2\")\nplt.title(\"Top 15 States by Job Count\")\nplt.xticks(rotation=30)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.savefig(\"figurestyj/top_states_job_count.png\", dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\n4 Heatmap: State × Industry (Cross Tab)\n\n\nCode\ncross_tab = pd.crosstab(df[\"STATE_NAME\"], df[\"NAICS_2022_2_NAME\"])\ntop_states = df[\"STATE_NAME\"].value_counts().head(10).index\ntop_industries = df[\"NAICS_2022_2_NAME\"].value_counts().head(6).index\nfiltered_heatmap = cross_tab.loc[top_states, top_industries]\n\nplt.figure(figsize=(10, 6))\nax = sns.heatmap(filtered_heatmap, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n\n\nxtick_labels = ax.get_xticklabels()\nwrapped_labels = [\n    '\\n'.join(textwrap.wrap(label.get_text(), width=15)) for label in xtick_labels\n]\nax.set_xticklabels(wrapped_labels, rotation=0)\nplt.xlabel(\"NAICS_2022_2_NAME\", fontsize=14) \npli.ylabel(\"State Name\", fontsize=14)\nplt.title(\"Top Industries in Top 10 States\")\nplt.tight_layout()\nplt.savefig(\"figurestyj/state_industry_heatmap.png\", dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\n5 Remote Job Distribution by State\n\n\nCode\nremote_by_state = df.groupby([\"STATE_NAME\", \"REMOTE_TYPE_NAME\"]).size().unstack(fill_value=0)\nremote_by_state = remote_by_state.loc[remote_by_state.sum(axis=1).sort_values(ascending=False).head(10).index]\n\nremote_by_state.plot(kind=\"bar\", stacked=True, figsize=(12, 6), colormap=\"Dark2\")\nplt.title(\"Remote Job Distribution by State\")\nplt.xlabel(\"State\")\nplt.ylabel(\"Number of Job Postings\")\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.xticks(rotation=30)\nplt.tight_layout()\nplt.savefig(\"figurestyj/remote_by_state.png\", dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
    "objectID": "modeling_analysis.html",
    "href": "modeling_analysis.html",
    "title": "Modeling & Analysis",
    "section": "",
    "text": "This section presents our machine learning models to analyze geographic and remote work patterns in the 2024 U.S. job market. We apply both unsupervised and supervised learning methods to gain insights into how job locations and remote types impact salaries and job classifications."
  },
  {
    "objectID": "modeling_analysis.html#supervised-learning-classification-predict-remote-type",
    "href": "modeling_analysis.html#supervised-learning-classification-predict-remote-type",
    "title": "Modeling & Analysis",
    "section": "2. Supervised Learning: Classification – Predict Remote Type",
    "text": "2. Supervised Learning: Classification – Predict Remote Type\nWe trained a Random Forest Classifier to predict REMOTE_TYPE_NAME based on job location (STATE_NAME), job category (SOC_2021_4), and experience (MAX_YEARS_EXPERIENCE). The model helps identify which job characteristics are more likely to be remote, hybrid, or onsite.\n\n\nCode\n# Import libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Prepare dataset\ndf_class = df[['STATE_NAME', 'SOC_2021_4', 'MAX_YEARS_EXPERIENCE', 'REMOTE_TYPE_NAME']].dropna()\ndf_class_encoded = pd.get_dummies(df_class, columns=['STATE_NAME', 'SOC_2021_4'])\n\nX = df_class_encoded.drop('REMOTE_TYPE_NAME', axis=1)\ny = df_class['REMOTE_TYPE_NAME']\n\n# Train/test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Train classifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\n# Print performance\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n\n\n# Visualize confusion matrix\nplt.figure(figsize=(6,5))\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\nplt.title(\"Confusion Matrix - Remote Work Type Classifier\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.tight_layout()\n\n# Save\nplt.savefig(\"figuresmurphy/confusion_matrix_remote_type.png\", dpi=300)\nplt.show()"
  },
  {
    "objectID": "modeling_analysis.html#supervised-learning-regression-predict-salary",
    "href": "modeling_analysis.html#supervised-learning-regression-predict-salary",
    "title": "Modeling & Analysis",
    "section": "3. Supervised Learning: Regression – Predict Salary",
    "text": "3. Supervised Learning: Regression – Predict Salary\nWe trained a Random Forest Regressor to predict AVERAGE_SALARY based on job attributes such as STATE_NAME, REMOTE_TYPE_NAME, SOC_2021_4, and MAX_YEARS_EXPERIENCE.\n\n\nCode\n# Step 1: Create AVERAGE_SALARY if not already in df\ndf['SALARY_FROM'] = pd.to_numeric(df['SALARY_FROM'], errors='coerce')\ndf['SALARY_TO'] = pd.to_numeric(df['SALARY_TO'], errors='coerce')\ndf['AVERAGE_SALARY'] = (df['SALARY_FROM'] + df['SALARY_TO']) / 2\n\n# Step 2: Drop rows with missing values in key columns\ndf_reg = df[['STATE_NAME', 'SOC_2021_4', 'REMOTE_TYPE_NAME', 'MAX_YEARS_EXPERIENCE', 'AVERAGE_SALARY']].dropna()\n\n# Step 3: One-hot encoding\ndf_reg_encoded = pd.get_dummies(df_reg, columns=['STATE_NAME', 'SOC_2021_4', 'REMOTE_TYPE_NAME'])\n\n# Step 4: Split X and y\nX = df_reg_encoded.drop('AVERAGE_SALARY', axis=1)\ny = df_reg_encoded['AVERAGE_SALARY']\n\n# Step 5: Train/test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Step 6: Train Random Forest Regressor\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(X_train, y_train)\n\n# Step 7: Predict\ny_pred = model.predict(X_test)\n\n# Step 8: Evaluate\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nprint(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\nprint(\"Mean Absolute Error:\", mean_absolute_error(y_test, y_pred))\nprint(\"R2 Score:\", r2_score(y_test, y_pred))\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(6,6))\nplt.scatter(y_test, y_pred, alpha=0.5)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\nplt.xlabel(\"Actual Salary\")\nplt.ylabel(\"Predicted Salary\")\nplt.title(\"Actual vs Predicted Salary\")\nplt.tight_layout()\nplt.savefig(\"figuresmurphy/actual_vs_predicted_salary.png\", dpi=300)\nplt.show()"
  },
  {
    "objectID": "skill_gap_analysis.html",
    "href": "skill_gap_analysis.html",
    "title": "Skill Gap Analysis",
    "section": "",
    "text": "This section presents the skill gap analysis between job requirements and available workforce skills."
  },
  {
    "objectID": "skill_gap_analysis.html#step-3-skill-gap-analysis",
    "href": "skill_gap_analysis.html#step-3-skill-gap-analysis",
    "title": "Skill Gap Analysis",
    "section": "",
    "text": "Team members’ current skills relevant to their selected IT career path：\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom collections import Counter\nimport re\n\n\n\n\nCode\ndf = pd.read_csv(\"data/lightcast_job_postings.csv\", encoding=\"utf-8\", on_bad_lines='skip')\n\n\n\n\nCode\nskills_data = {\n    \"Name\": [\"Eugenia\", \"Chenxi\", \"Xiangwen\"],\n    \"Python\": [3, 3, 5],\n    \"SQL\": [4, 2, 3],\n    \"Machine Learning\": [1, 2, 4],\n    \"Cloud Computing\": [3, 1, 2],\n    \"AWS\": [2, 4, 3]\n}\n\ndf_skills = pd.DataFrame(skills_data)\ndf_skills.set_index(\"Name\", inplace=True)\ndf_skills\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(df_skills, annot=True, cmap=\"coolwarm\", linewidths=0.5)\nplt.title(\"Team Skill Levels Heatmap\")\nplt.tight_layout()\nplt.savefig(\"figurestyj/team_skill_heatmap.png\")\nplt.show()\n\n\n\n\n\n\n\nCode\nfrom collections import Counter\n\ntop_skills = [\"Python\", \"SQL\", \"Machine Learning\", \"Cloud Computing\", \"Docker\", \"AWS\"]\njob_skill_counts = Counter(top_skills)\n\nfor skill in top_skills:\n    if skill not in df_skills.columns:\n        df_skills[skill] = 0  # Assume no knowledge in missing skills\n\ndf_skills\n\n\nimport os\nimport matplotlib.pyplot as plt\n\nos.makedirs(\"figureswxw\", exist_ok=True)\n\nteam_avg_skills = df_skills.mean()\n\nskills_to_plot = []\nfor skill in top_skills:\n    score = team_avg_skills[skill] if skill in team_avg_skills else 0\n    skills_to_plot.append(score)\n\nplt.figure(figsize=(10, 5))\nplt.bar(top_skills, skills_to_plot, color=\"skyblue\")\nplt.title(\"Average Team Skill Levels vs Top Skills\")\nplt.ylabel(\"Average Score\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.savefig(\"figurestyj/team_vs_industry_skills.png\")\nplt.show()\n\n\n\n\nCode\njob_descriptions = df[\"BODY\"].dropna().tolist()\nall_text = \" \".join(job_descriptions).lower()\n\nskills_keywords = [\"python\", \"sql\", \"machine learning\", \"cloud\", \"aws\", \"docker\", \"java\", \"excel\", \"r\", \"linux\"]\nskill_counts = Counter()\nfor skill in skills_keywords:\n    matches = re.findall(rf\"\\b{re.escape(skill)}\\b\", all_text)\n    skill_counts[skill] = len(matches)\n\ntop_skills = [skill for skill, count in skill_counts.most_common(5)]"
  },
  {
    "objectID": "nlp_methods.html",
    "href": "nlp_methods.html",
    "title": "NLP Methods",
    "section": "",
    "text": "This section uses natural language processing (NLP) to extract insights from job descriptions in the dataset, focusing on the most frequent skills and terms mentioned. We apply TF-IDF to identify distinguishing keywords, and generate a word cloud for intuitive visualization.\n\n1 Load and Preprocess Text Data\n\n\nCode\nimport pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\n\nCode\ndf = pd.read_csv(\"data/lightcast_job_postings.csv\", encoding=\"utf-8\", on_bad_lines='skip')\n\n\n\n\nCode\njob_desc = df[\"BODY\"].dropna().astype(str)\n\n\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r'\\d+', '', text)\n    text = re.sub(r'[^\\w\\s]', '', text)\n    return text\n\njob_desc_clean = job_desc.apply(clean_text)\n\n\n\n\n2 TF-IDF Analysis: Top Keywords\n\n\nCode\ntfidf = TfidfVectorizer(max_features=30, stop_words=\"english\")\ntfidf_matrix = tfidf.fit_transform(job_desc_clean)\n\nfeature_names = tfidf.get_feature_names_out()\nscores = tfidf_matrix.sum(axis=0).A1\n\ntfidf_df = pd.DataFrame({\"Term\": feature_names, \"Score\": scores})\ntfidf_df = tfidf_df.sort_values(by=\"Score\", ascending=False)\ntfidf_df.head(10)\n\n\n\n\nCode\nplt.figure(figsize=(12, 8))\nsns.barplot(data=tfidf_df.head(15), x=\"Score\", y=\"Term\", palette=\"viridis\")\nplt.title(\"Top TF-IDF Keywords in Job Descriptions\")\nplt.grid(axis='x', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.savefig(\"figurestyj/tfidf_keywords.png\", dpi=300,  bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\n3 Word Cloud Visualization\n\n\nCode\nfrom wordcloud import WordCloud\n\ntext_blob = \" \".join(job_desc_clean.tolist())\n\nwordcloud = WordCloud(width=1000, height=400, background_color=\"white\", max_words=100).generate(text_blob)\n\n\n\n\nCode\nplt.figure(figsize=(12, 8))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"Word Cloud of Job Description Terms\")\nplt.tight_layout()\nplt.savefig(\"figurestyj/jobdesc_wordcloud.png\", dpi=300)\nplt.show()\n\n\n\nThe results from the TF-IDF analysis reveal that the most distinctive keywords across job postings are “data”, “experience”, “business”, and “job”. These terms highlight the prevalence of data-centric roles in the current job market and underscore the significance of prior professional experience as a hiring criterion. Other frequently weighted terms include “skills”, “management”, and “team”, which indicate that employers are seeking candidates who possess both technical competencies and the ability to collaborate effectively within organizational structures.\nIn addition to the TF-IDF results, the word cloud visualization further enriches our understanding by emphasizing the recurring presence of phrases such as “bachelors degree”, “data analyst”, “support”, and “ability”. This aligns with expectations that many job postings include educational qualifications and role-specific technical terms. Moreover, the word cloud captures compliance-related language such as “gender identity”, “sexual orientation”, and “national origin”. These terms are commonly found in equal opportunity employment disclosures and reflect widespread adherence to diversity and inclusion standards in job advertisements.\nCollectively, the TF-IDF scores and the word cloud suggest several dominant themes within job descriptions. First, there is a consistent emphasis on technical qualifications, including skills in data analysis, cloud platforms, and tools such as Python and SAP. Second, postings frequently reference soft skills such as management, communication, and teamwork. Third, many job ads incorporate legal or standardized phrasing associated with hiring equity and regulatory compliance. Lastly, there is a strong focus on candidates’ educational background and accumulated experience.\nThese findings provide actionable guidance for job seekers, especially those pursuing roles in AI, tech, or data-related fields. Individuals are encouraged to showcase both technical expertise and interpersonal effectiveness in their resumes. Additionally, familiarity with standardized workplace language and professional communication expectations may enhance alignment with employer requirements."
  },
  {
    "objectID": "ml_analysis.html",
    "href": "ml_analysis.html",
    "title": "Modeling & Analysis",
    "section": "",
    "text": "This section presents our machine learning models to analyze geographic and remote work patterns in the 2024 U.S. job market. We apply both unsupervised and supervised learning methods to gain insights into how job locations and remote types impact salaries and job classifications."
  },
  {
    "objectID": "ml_analysis.html#supervised-learning-classification-predict-remote-type",
    "href": "ml_analysis.html#supervised-learning-classification-predict-remote-type",
    "title": "Modeling & Analysis",
    "section": "2.1 Supervised Learning: Classification – Predict Remote Type",
    "text": "2.1 Supervised Learning: Classification – Predict Remote Type\nTo understand what factors influence whether a job is remote, hybrid, or on-site, we trained a Random Forest Classifier using three features: STATE_NAME (location), SOC_2021_4 (job category), and MAX_YEARS_EXPERIENCE (seniority level).The model’s performance is summarized in a confusion matrix, which shows how accurately it distinguishes between different types of remote work arrangements.\n\n\nCode\n# Import libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Prepare dataset\ndf_class = df[['STATE_NAME', 'SOC_2021_4', 'MAX_YEARS_EXPERIENCE', 'REMOTE_TYPE_NAME']].dropna()\ndf_class_encoded = pd.get_dummies(df_class, columns=['STATE_NAME', 'SOC_2021_4'])\n\nX = df_class_encoded.drop('REMOTE_TYPE_NAME', axis=1)\ny = df_class['REMOTE_TYPE_NAME']\n\n# Train/test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Train classifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\n# Print performance\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n\n\n# Visualize confusion matrix\nplt.figure(figsize=(6,5))\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\nplt.title(\"Confusion Matrix - Remote Work Type Classifier\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.tight_layout()\n\n# Save\nplt.savefig(\"figuresmurphy/confusion_matrix_remote_type.png\", dpi=300)\nplt.show()\n\n\n\n\n\n\n\nInterpretation: The model accurately predicts class 3, but often misclassifies classes 0, 1, and 2 as 3, showing a bias toward the most common remote type. Job seekers should clearly state remote preferences, as nuanced roles may be missed by automated systems."
  },
  {
    "objectID": "ml_analysis.html#supervised-learning-regression-predict-salary",
    "href": "ml_analysis.html#supervised-learning-regression-predict-salary",
    "title": "Modeling & Analysis",
    "section": "2.2 Supervised Learning: Regression – Predict Salary",
    "text": "2.2 Supervised Learning: Regression – Predict Salary\nWe applied a Random Forest Regressor to estimate average salary using location, experience, remote type, and job category. The model captures complex patterns, highlighting how these factors shape compensation.\n\n\nCode\n# Step 1: Create AVERAGE_SALARY if not already in df\ndf['SALARY_FROM'] = pd.to_numeric(df['SALARY_FROM'], errors='coerce')\ndf['SALARY_TO'] = pd.to_numeric(df['SALARY_TO'], errors='coerce')\ndf['AVERAGE_SALARY'] = (df['SALARY_FROM'] + df['SALARY_TO']) / 2\n\n# Step 2: Drop rows with missing values in key columns\ndf_reg = df[['STATE_NAME', 'SOC_2021_4', 'REMOTE_TYPE_NAME', 'MAX_YEARS_EXPERIENCE', 'AVERAGE_SALARY']].dropna()\n\n# Step 3: One-hot encoding\ndf_reg_encoded = pd.get_dummies(df_reg, columns=['STATE_NAME', 'SOC_2021_4', 'REMOTE_TYPE_NAME'])\n\n# Step 4: Split X and y\nX = df_reg_encoded.drop('AVERAGE_SALARY', axis=1)\ny = df_reg_encoded['AVERAGE_SALARY']\n\n# Step 5: Train/test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Step 6: Train Random Forest Regressor\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(X_train, y_train)\n\n# Step 7: Predict\ny_pred = model.predict(X_test)\n\n# Step 8: Evaluate\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nprint(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\nprint(\"Mean Absolute Error:\", mean_absolute_error(y_test, y_pred))\nprint(\"R2 Score:\", r2_score(y_test, y_pred))\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(6,6))\nplt.scatter(y_test, y_pred, alpha=0.5)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\nplt.xlabel(\"Actual Salary\")\nplt.ylabel(\"Predicted Salary\")\nplt.title(\"Actual vs Predicted Salary\")\nplt.tight_layout()\nplt.savefig(\"figuresmurphy/actual_vs_predicted_salary.png\", dpi=300)\nplt.show()\n\n\n\n\n\n\n\nInterpretation: The plot shows a strong alignment between predicted and actual salaries, with most points near the red dashed line—indicating good model performance. Some deviations, especially in higher salary ranges, reflect the difficulty of predicting roles with greater variability in seniority and industry."
  },
  {
    "objectID": "ml_analysis.html#which-states-are-more-inclined-to-offer-remotehybridonsite-jobs",
    "href": "ml_analysis.html#which-states-are-more-inclined-to-offer-remotehybridonsite-jobs",
    "title": "Modeling & Analysis",
    "section": "2.3 Which states are more inclined to offer Remote/Hybrid/Onsite jobs?",
    "text": "2.3 Which states are more inclined to offer Remote/Hybrid/Onsite jobs?\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf_geo = df[['STATE_NAME', 'REMOTE_TYPE_NAME']].dropna()\n\n）\nstate_remote_counts = pd.crosstab(df_geo['STATE_NAME'], df_geo['REMOTE_TYPE_NAME'])\n\n# visualization\nstate_remote_counts.plot(kind='bar', stacked=True, figsize=(14,6))\nplt.title(\"Remote Work Type Distribution by State\")\nplt.xlabel(\"State\")\nplt.ylabel(\"Number of Job Postings\")\nplt.xticks(rotation=90)\nplt.tight_layout()\n\n\nplt.savefig(\"figuresmurphy/remote_type_by_state.png\", dpi=300)\nplt.show()\n\n\n\n\n\nRemote Work Type by State\n\n\nInterpretation: The bar chart shows that states like California, New York, and Texas have a high volume of job postings across all remote types. Remote roles are especially common in tech-focused states like California and Washington, while on-site jobs are more prevalent in states with stronger manufacturing industries."
  },
  {
    "objectID": "ml_analysis.html#choropleth-or-map-based-visualizations",
    "href": "ml_analysis.html#choropleth-or-map-based-visualizations",
    "title": "Modeling & Analysis",
    "section": "3.1 Choropleth or map-based visualizations",
    "text": "3.1 Choropleth or map-based visualizations\nWe used a choropleth map to visually represent the percentage of remote jobs per state. This spatial analysis can guide job seekers toward states with more flexible work environments.\n\n\nCode\nimport pandas as pd\nimport plotly.express as px\nimport pandas as pd\nimport plotly.express as px\n\n# 1. Create a full name -&gt; abbreviation mapping table\nstate_abbrev_map = {\n    'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR', 'California': 'CA',\n    'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE', 'Florida': 'FL', 'Georgia': 'GA',\n    'Hawaii': 'HI', 'Idaho': 'ID', 'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA',\n    'Kansas': 'KS', 'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD',\n    'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS',\n    'Missouri': 'MO', 'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV', 'New Hampshire': 'NH',\n    'New Jersey': 'NJ', 'New Mexico': 'NM', 'New York': 'NY', 'North Carolina': 'NC',\n    'North Dakota': 'ND', 'Ohio': 'OH', 'Oklahoma': 'OK', 'Oregon': 'OR', 'Pennsylvania': 'PA',\n    'Rhode Island': 'RI', 'South Carolina': 'SC', 'South Dakota': 'SD', 'Tennessee': 'TN',\n    'Texas': 'TX', 'Utah': 'UT', 'Vermont': 'VT', 'Virginia': 'VA', 'Washington': 'WA',\n    'West Virginia': 'WV', 'Wisconsin': 'WI', 'Wyoming': 'WY'\n}\n\n# 2. Add a column of state abbreviations\nremote_ratio_by_state['STATE_ABBR'] = remote_ratio_by_state['STATE_NAME'].map(state_abbrev_map)\n\nprint(remote_ratio_by_state[['STATE_NAME', 'STATE_ABBR']].head())\n\nfig = px.choropleth(\n    remote_ratio_by_state,\n    locations='STATE_ABBR',\n    locationmode='USA-states',\n    color='REMOTE_RATIO',\n    scope='usa',\n    color_continuous_scale='Blues',\n    title='Proportion of Remote Jobs by State (Using State Abbreviations)',\n    width=1000,      \n    height=600       \n)\nfig.write_image(\"figuresmurphy/choropleth.png\", scale=2)\n\nfig.show()\n\n\n\n\n\n\n\nInterpretation: The map highlights that coastal and urban states like California, New York, and Massachusetts have more remote roles. In contrast, Midwest and Southern states show fewer remote postings, likely due to a stronger focus on in-person or manufacturing jobs."
  },
  {
    "objectID": "ml_analysis.html#logistic-regression-binary-classification-remote-vs-non-remote",
    "href": "ml_analysis.html#logistic-regression-binary-classification-remote-vs-non-remote",
    "title": "Modeling & Analysis",
    "section": "3.2 Logistic Regression: Binary Classification (Remote vs Non-Remote)",
    "text": "3.2 Logistic Regression: Binary Classification (Remote vs Non-Remote)\nTo identify what influences whether a job is remote, we used a logistic regression model. The confusion matrix below summarizes how well the model classifies remote and non-remote roles.\n\n\nCode\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# 1. Preprocessing\ndf_logistic = df[['STATE_NAME', 'MAX_YEARS_EXPERIENCE', 'AVERAGE_SALARY', 'REMOTE_TYPE_NAME']].dropna()\n\ndf_logistic['REMOTE_TYPE_CLEANED'] = df_logistic['REMOTE_TYPE_NAME'].map({\n    'Remote': 'Remote',\n    'Hybrid Remote': 'Hybrid',\n    'Not Remote': 'Onsite'\n}).fillna('Onsite')\n\n# Create binary classification target\ndf_logistic['IS_REMOTE'] = df_logistic['REMOTE_TYPE_CLEANED'].apply(lambda x: 1 if x == 'Remote' else 0)\n\n# 2. One-hot encode state name\ndf_encoded = pd.get_dummies(df_logistic, columns=['STATE_NAME'], drop_first=True)\n\n# 3. Split data\nX = df_encoded.drop(['IS_REMOTE', 'REMOTE_TYPE_NAME', 'REMOTE_TYPE_CLEANED'], axis=1)\ny = df_encoded['IS_REMOTE']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# 4. Train model\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n# 5.  Evaluation\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n\n# 6. Visualize confusion matrix\nplt.figure(figsize=(6,4))\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\nplt.title(\"Confusion Matrix 热力图\")\nplt.xlabel(\"预测值 Predicted\")\nplt.ylabel(\"实际值 Actual\")\nplt.tight_layout()\nplt.savefig(\"figuresmurphy/logistic_confusion_matrix.png\", dpi=300)\nplt.show()\n\n\n\n\n\n\n\nInterpretation: The model accurately distinguishes remote from non-remote jobs, performing better on non-remote roles. Some misclassifications remain, so job seekers in flexible roles should clearly state their work preferences."
  },
  {
    "objectID": "ml_analysis.html#linear-regression",
    "href": "ml_analysis.html#linear-regression",
    "title": "Modeling & Analysis",
    "section": "4.1 Linear Regression",
    "text": "4.1 Linear Regression\nWe also used a linear regression model to predict average salary based on location, experience, and remote type. The actual vs. predicted plot and residuals histogram below show the model’s performance.\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\n\n# 1. 预处理 Preprocessing\ndf_reg = df[['STATE_NAME', 'MAX_YEARS_EXPERIENCE', 'REMOTE_TYPE_NAME', 'AVERAGE_SALARY']].dropna()\ndf_reg = df_reg[df_reg['AVERAGE_SALARY'] &lt; 300000]  # 去除极端值 Remove salary outliers\n\ndf_reg['REMOTE_TYPE_CLEANED'] = df_reg['REMOTE_TYPE_NAME'].map({\n    'Remote': 'Remote',\n    'Hybrid Remote': 'Hybrid',\n    'Not Remote': 'Onsite'\n}).fillna('Onsite')\n\n# 2. One-hot 编码 One-hot encode\ndf_reg_encoded = pd.get_dummies(df_reg, columns=['STATE_NAME', 'REMOTE_TYPE_CLEANED'], drop_first=True)\n\n# 3. 拆分数据 Split data\nX = df_reg_encoded.drop(['REMOTE_TYPE_NAME', 'AVERAGE_SALARY'], axis=1)\ny = df_reg_encoded['AVERAGE_SALARY']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# 4. 建模 Train model\nreg_model = LinearRegression()\nreg_model.fit(X_train, y_train)\ny_pred = reg_model.predict(X_test)\n\n# 5. 评估 Evaluation\nprint(\"MSE:\", mean_squared_error(y_test, y_pred))\nprint(\"R-squared:\", r2_score(y_test, y_pred))\n\n# 6. 可视化 - 实际 vs 预测工资散点图 Scatterplot of actual vs predicted\nplt.figure(figsize=(6, 5))\nplt.scatter(y_test, y_pred, alpha=0.5)\nplt.xlabel(\"实际工资 Actual Salary\")\nplt.ylabel(\"预测工资 Predicted Salary\")\nplt.title(\"实际 vs 预测工资散点图 (Actual vs. Predicted Salary)\")\nplt.tight_layout()\nplt.savefig(\"figuresmurphy/regression_actual_vs_predicted.png\", dpi=300)\nplt.show()\n\n# 7. 可视化 - 残差分布图 Residuals histogram\nresiduals = y_test - y_pred\nplt.figure(figsize=(6, 4))\nplt.hist(residuals, bins=30, color='orange', edgecolor='black')\nplt.title(\"残差分布图 Residuals Histogram\")\nplt.xlabel(\"残差 Residuals\")\nplt.ylabel(\"频率 Frequency\")\nplt.tight_layout()\nplt.savefig(\"figuresmurphy/regression_residuals.png\", dpi=300)\nplt.show()\n\n\n\n\n\n\n\nInterpretation: The plot shows a clear linear trend but with greater dispersion compared to the random forest model, indicating that linear regression struggles to capture more complex salary patterns.\n\n\n\n\n\nInterpretation: The residuals histogram is roughly normal and centered around zero, suggesting no major bias. However, the spread shows that predictions can vary by several thousand dollars based on the input features."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AD688 Summer25 Group Project - Job Market Analysis - Group 4",
    "section": "",
    "text": "Topic: Geographic and Remote Work Analysis\n\nWhich cities or states have the highest job growth for AI vs. non-AI careers?\nAre remote jobs increasing or decreasing across industries?\nDo tech hubs (e.g., Silicon Valley, Austin, Boston) still dominate hiring, or are other locations emerging?\nHow do urban vs. rural job markets differ for AI and non-AI careers?"
  },
  {
    "objectID": "skill_gap_analysis.html#team-members-current-skill-levels",
    "href": "skill_gap_analysis.html#team-members-current-skill-levels",
    "title": "Skill Gap Analysis",
    "section": "1.1 Team Members’ Current Skill Levels",
    "text": "1.1 Team Members’ Current Skill Levels\n\nskills_data = {\n    \"Name\": [\"Eugenia\", \"Chenxi\", \"Xiangwen\"],\n    \"Python\": [3, 3, 5],\n    \"SQL\": [4, 2, 3],\n    \"Machine Learning\": [1, 2, 4],\n    \"Cloud Computing\": [3, 1, 2],\n    \"AWS\": [2, 4, 3]\n}\n\n\ndf_skills = pd.DataFrame(skills_data)\ndf_skills.set_index(\"Name\", inplace=True)\ndf_skills"
  },
  {
    "objectID": "skill_gap_analysis.html#current-skill-heatmap",
    "href": "skill_gap_analysis.html#current-skill-heatmap",
    "title": "Skill Gap Analysis",
    "section": "1.2 Current Skill Heatmap",
    "text": "1.2 Current Skill Heatmap\n\n\nCode\nplt.figure(figsize=(10, 6))\nsns.heatmap(df_skills, annot=True, cmap=\"coolwarm\", linewidths=0.5)\nplt.xlabel(\"Technical Skills\", fontsize=14)       \nplt.ylabel(\"Team Members\", fontsize=14)   \nplt.title(\"Team Skill Levels Heatmap\")\nplt.tight_layout()\nplt.savefig(\"figurestyj/team_skill_heatmap.png\", dpi=300,  bbox_inches='tight')\nplt.show()"
  },
  {
    "objectID": "skill_gap_analysis.html#personalized-upskilling-recommendations",
    "href": "skill_gap_analysis.html#personalized-upskilling-recommendations",
    "title": "Skill Gap Analysis",
    "section": "1.3 Personalized Upskilling Recommendations",
    "text": "1.3 Personalized Upskilling Recommendations\n\n\nCode\nrecommendations = {\n    \"Eugenia\": [\"Machine Learning\", \"AWS\"],\n    \"Chenxi\": [\"Cloud Computing\", \"Python\"],\n    \"Xiangwen\": [\"Docker\", \"Cloud Computing\"]\n}\n\ndf_recommend = pd.DataFrame([\n    {\"Name\": name, \"Recommended Skills to Improve\": \", \".join(skills)}\n    for name, skills in recommendations.items()\n])\ndf_recommend\n\n\nTo close the observed skill gaps, we provide the following individualized upskilling plans:\nEugenia is advised to focus on Machine Learning and AWS to better align with industry expectations.\nChenxi would benefit from strengthening her Cloud Computing and Python skills.\nXiangwen is recommended to enhance his proficiency in Docker and Cloud-related technologies.\nThese suggestions are based on both internal team comparisons and external job market demands, and they aim to improve overall team balance and job readiness."
  },
  {
    "objectID": "skill_gap_analysis.html#compute-average-team-skills-vs-industry-expectations",
    "href": "skill_gap_analysis.html#compute-average-team-skills-vs-industry-expectations",
    "title": "Skill Gap Analysis",
    "section": "2.1 Compute Average Team Skills vs Industry Expectations",
    "text": "2.1 Compute Average Team Skills vs Industry Expectations\n\n\nCode\ntop_skills = [\"Python\", \"SQL\", \"Machine Learning\", \"Cloud Computing\", \"Docker\", \"AWS\"]\njob_skill_counts = Counter(top_skills)\n\nfor skill in top_skills:\n    if skill not in df_skills.columns:\n        df_skills[skill] = 0  # Assume no knowledge in missing skills\n\ndf_skills\n\n\nos.makedirs(\"figurestyj\", exist_ok=True)\n\nteam_avg_skills = df_skills.mean()\n\nskills_to_plot = []\nfor skill in top_skills:\n    score = team_avg_skills[skill] if skill in team_avg_skills else 0\n    skills_to_plot.append(score)\n\nplt.figure(figsize=(10, 5))\nplt.bar(top_skills, skills_to_plot, color=\"skyblue\")\nplt.title(\"Average Team Skill Levels vs Top Skills\")\nplt.xlabel(\"Technical Skills\", fontsize=14)  \nplt.ylabel(\"Average Score\",fontsize=14 )\nplt.xticks(rotation=45)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.savefig(\"figurestyj/team_vs_industry_skills.png\", dpi=300,  bbox_inches='tight')\nplt.show()"
  }
]