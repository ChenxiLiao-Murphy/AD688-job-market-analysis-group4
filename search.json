[
  {
    "objectID": "ml_methods.html",
    "href": "ml_methods.html",
    "title": "ML Methods",
    "section": "",
    "text": "ML Methods\nThis page describes the machine learning methods applied to analyze the job market data.\nContent will be added here."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "nlp_methods.html",
    "href": "nlp_methods.html",
    "title": "NLP Methods",
    "section": "",
    "text": "NLP Methods\nThis page introduces the natural language processing (NLP) techniques used in this project.\nContent will be added here."
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Exploratory Data Analysis\nThis page summarizes the exploratory data analysis (EDA) of the dataset, including key trends and patterns.\nContent will be added here."
  },
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "This page provides an overview of the data cleaning and initial exploration process for the job market dataset.\nContent will be added here.\n\n\n\n\nCode\nimport pandas as pd\ndf1 = pd.read_csv(\"./data/lightcast_job_postings.csv\")\n\n\n\n\nCode\ndf1.head()\ndf1.info()\ndf1.describe()\n\n\n\n\n\n\n\nCode\ndf1.columns.tolist()\nprint(df1.columns.tolist())\n\n\n\n\n\n\n# Define columns that are irrelevant or redundant for our analysis\ncolumns_to_drop = [\n    # Tracking and metadata\n    \"ID\", \"LAST_UPDATED_DATE\", \"LAST_UPDATED_TIMESTAMP\", \"DUPLICATES\",\n    \"URL\", \"ACTIVE_URLS\", \"ACTIVE_SOURCES_INFO\", \"SOURCE_TYPES\", \"SOURCES\",\n\n    # Company raw info\n    \"COMPANY_RAW\", \"COMPANY_IS_STAFFING\",\n\n    # Raw or text-heavy fields\n    \"TITLE_RAW\", \"BODY\",\n\n    # Modeled / derived fields\n    \"MODELED_EXPIRED\", \"MODELED_DURATION\",\n\n    # Educational levels (redundant versions)\n    \"EDUCATION_LEVELS\", \"EDUCATION_LEVELS_NAME\",\n    \"MIN_EDULEVELS\", \"MIN_EDULEVELS_NAME\", \"MAX_EDULEVELS\",\n\n    # Redundant NAICS / SOC codes\n    \"NAICS_2022_2\", \"NAICS_2022_2_NAME\",\n    \"NAICS_2022_3\", \"NAICS_2022_3_NAME\",\n    \"SOC_2\", \"SOC_3\", \"SOC_5\"\n]\n\n# Drop columns, ignore if a column is missing\ndf1.drop(columns=columns_to_drop, inplace=True, errors=\"ignore\")\n\n# Display the first few rows to confirm\ndf1.head()\n\n\n\n\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nos.makedirs(\"figureswxw\", exist_ok=True)\n\n\n\n\nCode\nimport missingno as msno\nimport matplotlib.pyplot as plt\nmsno.heatmap(df1)\n\nplt.title(\"Missing Values Heatmap\")\nplt.tight_layout()\nplt.savefig(\"figureswxw/missing_values_heatmap.png\", dpi=300)\nplt.show()\n\n\n\n\n\n\n\n\n\nCode\n# Drop columns with &gt;50% missing values\ndf1.dropna(axis=1, thresh=len(df1) * 0.5, inplace=True)\n\n\nif \"SALARY\" in df1.columns:\n    df1[\"SALARY\"] = df1[\"SALARY\"].fillna(df1[\"SALARY\"].median())\n\n    df1[\"DURATION\"] = df1[\"DURATION\"].fillna(df1[\"DURATION\"].median())\n\ncategorical_columns = [\"REMOTE_TYPE_NAME\", \"COMPANY_NAME\", \"MAX_EDULEVELS_NAME\"]\n\nfor col in categorical_columns:\n    if col in df1.columns:\n        df1[col] = df1[col].fillna(\"Unknown\")\n\n\ndf1.info()\n\n\n\n\n\n\ndf1.drop_duplicates(subset=[\"TITLE_CLEAN\", \"COMPANY_NAME\", \"CITY_NAME\", \"POSTED\"], inplace=True)\n\ndf1[\"REMOTE_TYPE_NAME\"].value_counts(dropna=False)\ndf1[\"EMPLOYMENT_TYPE_NAME\"].value_counts(dropna=False)\n\n\n\nCode\n#improve\n\ndf1[\"EMPLOYMENT_TYPE_NAME\"] = df1[\"EMPLOYMENT_TYPE_NAME\"].replace({\n    \"Part-time (â‰¤ 32 hours)\": \"Part-time (≤ 32 hours)\",\n    \"Part-time / full-time\": \"Part-time / Full-time\"\n})\ndf1[\"EMPLOYMENT_TYPE_NAME\"] = df1[\"EMPLOYMENT_TYPE_NAME\"].fillna(\"Unknown\")\ndf1[\"EMPLOYMENT_TYPE_NAME\"].value_counts()"
  },
  {
    "objectID": "data_cleaning.html#load-dataset",
    "href": "data_cleaning.html#load-dataset",
    "title": "Data Cleaning",
    "section": "",
    "text": "Code\nimport pandas as pd\ndf1 = pd.read_csv(\"./data/lightcast_job_postings.csv\")\n\n\n\n\nCode\ndf1.head()\ndf1.info()\ndf1.describe()"
  },
  {
    "objectID": "data_cleaning.html#check-columns-information",
    "href": "data_cleaning.html#check-columns-information",
    "title": "Data Cleaning",
    "section": "",
    "text": "Code\ndf1.columns.tolist()\nprint(df1.columns.tolist())"
  },
  {
    "objectID": "data_cleaning.html#dropping-unnecessary-columns",
    "href": "data_cleaning.html#dropping-unnecessary-columns",
    "title": "Data Cleaning",
    "section": "",
    "text": "# Define columns that are irrelevant or redundant for our analysis\ncolumns_to_drop = [\n    # Tracking and metadata\n    \"ID\", \"LAST_UPDATED_DATE\", \"LAST_UPDATED_TIMESTAMP\", \"DUPLICATES\",\n    \"URL\", \"ACTIVE_URLS\", \"ACTIVE_SOURCES_INFO\", \"SOURCE_TYPES\", \"SOURCES\",\n\n    # Company raw info\n    \"COMPANY_RAW\", \"COMPANY_IS_STAFFING\",\n\n    # Raw or text-heavy fields\n    \"TITLE_RAW\", \"BODY\",\n\n    # Modeled / derived fields\n    \"MODELED_EXPIRED\", \"MODELED_DURATION\",\n\n    # Educational levels (redundant versions)\n    \"EDUCATION_LEVELS\", \"EDUCATION_LEVELS_NAME\",\n    \"MIN_EDULEVELS\", \"MIN_EDULEVELS_NAME\", \"MAX_EDULEVELS\",\n\n    # Redundant NAICS / SOC codes\n    \"NAICS_2022_2\", \"NAICS_2022_2_NAME\",\n    \"NAICS_2022_3\", \"NAICS_2022_3_NAME\",\n    \"SOC_2\", \"SOC_3\", \"SOC_5\"\n]\n\n# Drop columns, ignore if a column is missing\ndf1.drop(columns=columns_to_drop, inplace=True, errors=\"ignore\")\n\n# Display the first few rows to confirm\ndf1.head()"
  },
  {
    "objectID": "data_cleaning.html#handling-missing-values",
    "href": "data_cleaning.html#handling-missing-values",
    "title": "Data Cleaning",
    "section": "",
    "text": "Code\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nos.makedirs(\"figureswxw\", exist_ok=True)\n\n\n\n\nCode\nimport missingno as msno\nimport matplotlib.pyplot as plt\nmsno.heatmap(df1)\n\nplt.title(\"Missing Values Heatmap\")\nplt.tight_layout()\nplt.savefig(\"figureswxw/missing_values_heatmap.png\", dpi=300)\nplt.show()\n\n\n\n\n\n\n\n\n\nCode\n# Drop columns with &gt;50% missing values\ndf1.dropna(axis=1, thresh=len(df1) * 0.5, inplace=True)\n\n\nif \"SALARY\" in df1.columns:\n    df1[\"SALARY\"] = df1[\"SALARY\"].fillna(df1[\"SALARY\"].median())\n\n    df1[\"DURATION\"] = df1[\"DURATION\"].fillna(df1[\"DURATION\"].median())\n\ncategorical_columns = [\"REMOTE_TYPE_NAME\", \"COMPANY_NAME\", \"MAX_EDULEVELS_NAME\"]\n\nfor col in categorical_columns:\n    if col in df1.columns:\n        df1[col] = df1[col].fillna(\"Unknown\")\n\n\ndf1.info()"
  },
  {
    "objectID": "data_cleaning.html#remove-duplicates",
    "href": "data_cleaning.html#remove-duplicates",
    "title": "Data Cleaning",
    "section": "",
    "text": "df1.drop_duplicates(subset=[\"TITLE_CLEAN\", \"COMPANY_NAME\", \"CITY_NAME\", \"POSTED\"], inplace=True)\n\ndf1[\"REMOTE_TYPE_NAME\"].value_counts(dropna=False)\ndf1[\"EMPLOYMENT_TYPE_NAME\"].value_counts(dropna=False)\n\n\n\nCode\n#improve\n\ndf1[\"EMPLOYMENT_TYPE_NAME\"] = df1[\"EMPLOYMENT_TYPE_NAME\"].replace({\n    \"Part-time (â‰¤ 32 hours)\": \"Part-time (≤ 32 hours)\",\n    \"Part-time / full-time\": \"Part-time / Full-time\"\n})\ndf1[\"EMPLOYMENT_TYPE_NAME\"] = df1[\"EMPLOYMENT_TYPE_NAME\"].fillna(\"Unknown\")\ndf1[\"EMPLOYMENT_TYPE_NAME\"].value_counts()"
  },
  {
    "objectID": "data_cleaning.html#remote-type-distribution",
    "href": "data_cleaning.html#remote-type-distribution",
    "title": "Data Cleaning",
    "section": "2.1 Remote Type distribution",
    "text": "2.1 Remote Type distribution\n\n\nCode\nremote_counts = df1[\"REMOTE_TYPE_NAME\"].value_counts()\n\nplt.figure(figsize=(10,6))\nsns.barplot(\n    x=remote_counts.index, \n    y=remote_counts.values, \n    palette=\"Set2\"\n)\nplt.title(\"Remote Type Distribution\")\nplt.ylabel(\"Number of Job Postings\")\nplt.xlabel(\"Remote Type\")\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.savefig(\"figureswxw/remote_type_distribution.png\", dpi=300)\nplt.show()\n\n\n\n\n\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf1[\"IS_AI\"] = df1[\"NAICS_2022_6_NAME\"].fillna(\"\").str.contains(\"AI|Artificial Intelligence\", case=False) | \\\n               df1[\"LOT_OCCUPATION\"].fillna(\"\").str.contains(\"AI|Artificial Intelligence\", case=False)\n\ndf1[\"IS_AI\"] = df1[\"IS_AI\"].map({True: \"AI\", False: \"Non-AI\"})"
  },
  {
    "objectID": "data_cleaning.html#top-10-states-ai-vs-non-ai-job-postings",
    "href": "data_cleaning.html#top-10-states-ai-vs-non-ai-job-postings",
    "title": "Data Cleaning",
    "section": "2.2 Top 10 states : AI vs Non-AI Job Postings",
    "text": "2.2 Top 10 states : AI vs Non-AI Job Postings\n\n\nCode\ntop_states = df1[\"STATE_NAME\"].value_counts().head(10).index\ndf_top_states = df1[df1[\"STATE_NAME\"].isin(top_states)]\n\npivot_states = df_top_states.groupby([\"STATE_NAME\", \"IS_AI\"]).size().unstack(fill_value=0)\n\npivot_states.plot(kind=\"bar\", stacked=True, figsize=(12,6), colormap=\"Set3\")\nplt.title(\"Top 10 States: AI vs Non-AI Job Postings\")\nplt.ylabel(\"Number of Job Postings\")\nplt.xlabel(\"State\")\nplt.xticks(rotation=30)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.savefig(\"figureswxw/top_states_ai_nonai.png\", dpi=300)\nplt.show()"
  },
  {
    "objectID": "data_cleaning.html#top-10-cities-ai-vs-non-ai-job-postings",
    "href": "data_cleaning.html#top-10-cities-ai-vs-non-ai-job-postings",
    "title": "Data Cleaning",
    "section": "2.3 Top 10 cities: AI vs Non-AI Job Postings",
    "text": "2.3 Top 10 cities: AI vs Non-AI Job Postings\n\n\nCode\ntop_cities = df1[\"CITY_NAME\"].value_counts().head(10).index\ndf_top_cities = df1[df1[\"CITY_NAME\"].isin(top_cities)]\n\npivot_cities = df_top_cities.groupby([\"CITY_NAME\", \"IS_AI\"]).size().unstack(fill_value=0)\n\npivot_cities.plot(kind=\"bar\", stacked=True, figsize=(12,6), colormap=\"Set1\")\nplt.title(\"Top 10 Cities: AI vs Non-AI Job Postings\")\nplt.ylabel(\"Number of Job Postings\")\nplt.xlabel(\"City\")\nplt.xticks(rotation=30)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.savefig(\"figureswxw/top_cities_ai_nonai.png\", dpi=300)\nplt.show()"
  },
  {
    "objectID": "data_cleaning.html#time-trend-of-remote-work-types",
    "href": "data_cleaning.html#time-trend-of-remote-work-types",
    "title": "Data Cleaning",
    "section": "2.4 Time Trend of Remote Work Types",
    "text": "2.4 Time Trend of Remote Work Types\n\n\nCode\nif \"POSTED\" in df1.columns:\n    df1[\"POSTED_DATE\"] = pd.to_datetime(df1[\"POSTED\"], errors='coerce')\n    df1 = df1.dropna(subset=[\"POSTED_DATE\"])\n    df1[\"POSTED_MONTH\"] = df1[\"POSTED_DATE\"].dt.to_period(\"M\")\n    \n    trend = df1.groupby([\"POSTED_MONTH\", \"REMOTE_TYPE_NAME\"]).size().unstack(fill_value=0)\n    \n    trend.plot(figsize=(14,7))\n    plt.title(\"Remote Work Trends Over Time\", fontsize=14)\n    plt.ylabel(\"Number of Job Postings\")\n    plt.xlabel(\"Month\")\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.savefig(\"figureswxw/remote_trend_over_time.png\", dpi=300)\n    plt.show()\nelse:\n    print(\"POSTED column not found in dataset.\")"
  },
  {
    "objectID": "data_cleaning.html#tech-hubs-vs-other-locations-hiring-trends",
    "href": "data_cleaning.html#tech-hubs-vs-other-locations-hiring-trends",
    "title": "Data Cleaning",
    "section": "2.5 Tech Hubs vs Other Locations Hiring Trends",
    "text": "2.5 Tech Hubs vs Other Locations Hiring Trends\n\n\nCode\ndf1[\"IS_HUB\"] = df1[\"CITY_NAME\"].apply(lambda x: \"Hub\" if x in [\"San Francisco\", \"Austin\", \"Boston\"] else \"Other\")\n\npivot_hub = df1.groupby([\"POSTED_MONTH\", \"IS_HUB\"]).size().unstack(fill_value=0)\n\npivot_hub.plot(figsize=(14,7))\nplt.title(\"Hiring Trends: Tech Hubs vs Other Locations\")\nplt.ylabel(\"Number of Job Postings\")\nplt.xlabel(\"Month\")\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.savefig(\"figureswxw/techhub_vs_other_trend.png\", dpi=300)\nplt.show()"
  },
  {
    "objectID": "data_cleaning.html#remote-job-trend-by-industry",
    "href": "data_cleaning.html#remote-job-trend-by-industry",
    "title": "Data Cleaning",
    "section": "2.6 Remote Job Trend by Industry",
    "text": "2.6 Remote Job Trend by Industry\n\n\nCode\ntop_industries = (\n    df1.groupby(\"NAICS_2022_6_NAME\").size()\n    .sort_values(ascending=False)\n    .head(10)\n    .index\n)\n\n\ndf_top_ind = df1[df1[\"NAICS_2022_6_NAME\"].isin(top_industries)]\n\n\ndf_top_ind[\"POSTED_DATE\"] = pd.to_datetime(df_top_ind[\"POSTED\"], errors='coerce')\ndf_top_ind = df_top_ind.dropna(subset=[\"POSTED_DATE\"])\ndf_top_ind[\"POSTED_MONTH\"] = df_top_ind[\"POSTED_DATE\"].dt.to_period(\"M\")\n\n\npivot = df_top_ind.groupby([\"POSTED_MONTH\", \"NAICS_2022_6_NAME\"]).size().unstack(fill_value=0)\n\npivot.plot(figsize=(14,7))\nplt.title(\"Remote Job Trends by Top 5 Industries Over Time\", fontsize=14)\nplt.ylabel(\"Number of Job Postings\")\nplt.xlabel(\"Month\")\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.legend(\n    title=\"NAICS_2022_6_NAME\",\n    loc='upper center',\n    bbox_to_anchor=(0.5, -0.15),  \n    ncol=2,                       \n    frameon=False\n)\nplt.tight_layout()\nplt.savefig(\"figureswxw/remote_trend_top5_industry.png\", dpi=300)\nplt.show()"
  },
  {
    "objectID": "data_cleaning.html#urbanrural-region-ai-vs-non-ai-job-postings",
    "href": "data_cleaning.html#urbanrural-region-ai-vs-non-ai-job-postings",
    "title": "Data Cleaning",
    "section": "2.7 Urban/Rural Region: AI vs Non-AI Job Postings",
    "text": "2.7 Urban/Rural Region: AI vs Non-AI Job Postings\n\n\nCode\nurban_cities = [\n    \"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"San Francisco\",\n    \"Austin\", \"Boston\", \"Dallas\", \"Seattle\", \"Washington\", \"Atlanta\"\n]\n\ndf1[\"CITY_NAME_CLEAN\"] = df1[\"CITY_NAME\"].str.split(\",\").str[0].str.strip().str.title()\n\ndf1[\"Urban_Rural\"] = df1[\"CITY_NAME_CLEAN\"].apply(\n    lambda x: \"Urban\" if x in urban_cities else \"Rural\"\n)\n\nprint(df1[\"Urban_Rural\"].value_counts())\n\n\n\n2.7.1 Stacked Bar Chart\nrequired_columns = [“Urban_Rural”, “IS_AI”] missing_columns = [col for col in required_columns if col not in df1.columns]\nif not missing_columns: import matplotlib.pyplot as plt\npivot_urban = df1.groupby([\"Urban_Rural\", \"IS_AI\"]).size().unstack(fill_value=0)\n\n\npivot_urban.plot(\n    kind=\"bar\",\n    stacked=True,\n    figsize=(8, 5),\n    color=[\"#ff9999\", \"#66b3ff\"],\n    edgecolor=\"black\"\n)\n\nplt.title(\"Urban and Rural Regions: AI vs Non-AI Job Postings\", fontsize=14)\nplt.ylabel(\"Number of Job Postings\")\nplt.xlabel(\"Region Type\")\nplt.xticks(rotation=0)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.savefig(\"figureswxw/urban_rural_ai_nonai_bar.png\", dpi=300)\nplt.show()\nelse: print(f”The following required columns are missing: {missing_columns}. Please check your dataset.”) ```"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AD688 Summer25 Group Project - Job Market Analysis - Group 4",
    "section": "",
    "text": "Topic: Geographic and Remote Work Analysis\n\nWhich cities or states have the highest job growth for AI vs. non-AI careers?\nAre remote jobs increasing or decreasing across industries?\nDo tech hubs (e.g., Silicon Valley, Austin, Boston) still dominate hiring, or are other locations emerging?\nHow do urban vs. rural job markets differ for AI and non-AI careers?"
  },
  {
    "objectID": "skill_gap_analysis.html",
    "href": "skill_gap_analysis.html",
    "title": "Skill Gap Analysis",
    "section": "",
    "text": "Skill Gap Analysis\nThis page presents the skill gap analysis between job requirements and available workforce skills.\nContent will be added here."
  }
]